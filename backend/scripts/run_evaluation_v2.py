"""
Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô Evaluation
‡πÉ‡∏ä‡πâ AIServiceForEvaluation ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å

‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
    python backend/scripts/run_evaluation.py --mode quick
    python backend/scripts/run_evaluation.py --mode full
    python backend/scripts/run_evaluation.py --mode full --no-save
"""

import sys
import os
import asyncio
import json
import argparse
from pathlib import Path
from datetime import datetime

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path
sys.path.append(str(Path(__file__).parent.parent))

# ‡∏£‡∏∞‡∏ö‡∏ö Evaluation (‡πÉ‡∏´‡∏°‡πà)
from app.services.evaluation_service import EvaluationService
from app.services.evaluation_test_data import EvaluationTestData
from app.services.ai_service_for_evaluation import AIServiceForEvaluation  # ‚Üê ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà!

# ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏î‡∏¥‡∏°)
from app.services.rag_service import RAGService
from app.services.tax_service import TaxService, TaxCalculationRequest  # ‚Üê import ‡∏à‡∏≤‡∏Å tax_service


async def evaluate_ai_recommendations(verbose: bool = True, save_logs: bool = True):
    """
    ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å AI ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    
    Args:
        verbose: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° debug
        save_logs: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å logs ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    """
    print("\n" + "="*80)
    print("üöÄ STARTING AI TAX ADVISOR EVALUATION")
    print("="*80)
    print(f"‚öôÔ∏è  Verbose: {verbose}")
    print(f"üíæ Save logs: {save_logs}")
    print("="*80 + "\n")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á services
    evaluator = EvaluationService()
    ai_service = AIServiceForEvaluation(verbose=verbose, save_to_file=save_logs)  # ‚Üê ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà!
    rag_service = RAGService()
    tax_service = TaxService()
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á log directory
    if save_logs:
        print(f"üìÇ Logs will be saved to: {ai_service.log_dir}\n")
    
    # ‡∏î‡∏∂‡∏á test cases
    test_cases = EvaluationTestData.get_all_test_cases()
    
    all_results = []
    
    for idx, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*80}")
        print(f"üìã TEST CASE {idx} / {len(test_cases)}")
        print(f"{'='*80}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á request
        request = TaxCalculationRequest(**test_case['input'])
        print(f"\nüí∞ ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ: {request.gross_income:,} ‡∏ö‡∏≤‡∏ó")
        print(f"üéØ ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {request.risk_tolerance}")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏©‡∏µ
        tax_result = tax_service.calculate_tax(request)
        print(f"üí∏ ‡∏†‡∏≤‡∏©‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏à‡πà‡∏≤‡∏¢: {tax_result.tax_amount:,} ‡∏ö‡∏≤‡∏ó")
        
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG
        query = f"""‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ {request.gross_income} ‡∏ö‡∏≤‡∏ó ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á {request.risk_tolerance} ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏†‡∏≤‡∏©‡∏µ"""
        
        try:
            # ‡∏•‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô
            retrieved_docs = await rag_service.retrieve_relevant_documents(query)
        except TypeError:
            # ‡∏ñ‡πâ‡∏≤ error ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ rag_service ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ parameter ‡πÄ‡∏û‡∏¥‡πà‡∏°
            # ‡∏•‡∏≠‡∏á‡∏™‡πà‡∏á tax_result ‡∏î‡πâ‡∏ß‡∏¢
            try:
                retrieved_docs = await rag_service.retrieve_relevant_documents(query, tax_result)
            except:
                # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á error ‡πÉ‡∏ä‡πâ mock data
                print("‚ö†Ô∏è  RAG service error, using mock data")
                retrieved_docs = [type('obj', (object,), {'page_content': 'RMF ‡∏•‡∏î‡∏´‡∏¢‡πà‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏µ‡πÑ‡∏î‡πâ 30% ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ'})()]
        
        
        # ‡∏î‡∏∂‡∏á content ‡∏à‡∏≤‡∏Å documents (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö)
        context_parts = []
        for doc in retrieved_docs:
            if hasattr(doc, 'page_content'):
                context_parts.append(doc.page_content)
            elif hasattr(doc, 'content'):
                context_parts.append(doc.content)
            elif isinstance(doc, str):
                context_parts.append(doc)
            elif isinstance(doc, dict):
                context_parts.append(doc.get('content', '') or doc.get('page_content', ''))
        
        context = "\n\n".join(context_parts) if context_parts else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG"
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI (‡∏ï‡∏±‡∏ß‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á raw response)
        print(f"\nü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å AI...\n")
        ai_recommendations, raw_response = await ai_service.generate_recommendations(
            request, tax_result, context, test_case_id=idx
        )
        
        print(f"‚úÖ AI ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ {len(ai_recommendations)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
        expected_recommendations = test_case['expected_recommendations']
        
        case_results = {
            'test_case': idx,
            'input': test_case['input'],
            'raw_response': raw_response,  # ‚Üê ‡πÄ‡∏Å‡πá‡∏ö raw response ‡∏î‡πâ‡∏ß‡∏¢
            'ai_recommendations_count': len(ai_recommendations),
            'expected_recommendations_count': len(expected_recommendations),
            'evaluations': []
        }
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡∏•‡∏∞‡∏ï‡∏±‡∏ß
        for ai_rec_idx, ai_rec in enumerate(ai_recommendations[:3], 1):
            if ai_rec_idx <= len(expected_recommendations):
                expected_rec = expected_recommendations[ai_rec_idx - 1]
                
                print(f"\n{'‚îÄ'*80}")
                print(f"üìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà {ai_rec_idx}")
                print(f"{'‚îÄ'*80}")
                
                print(f"\n‚úÖ Expected: {expected_rec['strategy']}")
                print(f"ü§ñ AI:       {ai_rec.get('strategy', 'N/A')}")
                
                # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
                eval_result = evaluator.evaluate_recommendation(
                    expected_rec, ai_rec
                )
                
                evaluator.print_evaluation_report(eval_result)
                
                case_results['evaluations'].append({
                    'recommendation_index': ai_rec_idx,
                    'expected': expected_rec,
                    'ai': ai_rec,
                    'scores': eval_result
                })
        
        all_results.append(case_results)
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏£‡∏ß‡∏°
    print(f"\n{'='*80}")
    print("üìà OVERALL SUMMARY")
    print(f"{'='*80}")
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
    all_rouge1 = []
    all_rouge2 = []
    all_rougeL = []
    all_bleu4 = []
    all_investment_accuracy = []
    all_tax_saving_accuracy = []
    
    for case in all_results:
        for eval_item in case['evaluations']:
            scores = eval_item['scores']
            all_rouge1.append(scores.get('rouge1_f1', 0))
            all_rouge2.append(scores.get('rouge2_f1', 0))
            all_rougeL.append(scores.get('rougeL_f1', 0))
            all_bleu4.append(scores.get('bleu4', 0))
            all_investment_accuracy.append(scores.get('investment_amount_accuracy', 0))
            all_tax_saving_accuracy.append(scores.get('tax_saving_accuracy', 0))
    
    import numpy as np
    
    print(f"\nüî¥ ROUGE Scores (Average):")
    print(f"  ROUGE-1 F1: {np.mean(all_rouge1):.4f}")
    print(f"  ROUGE-2 F1: {np.mean(all_rouge2):.4f}")
    print(f"  ROUGE-L F1: {np.mean(all_rougeL):.4f}")
    
    print(f"\nüîµ BLEU Score (Average):")
    print(f"  BLEU-4: {np.mean(all_bleu4):.4f}")
    
    print(f"\nüí∞ Numeric Accuracy (Average):")
    print(f"  Investment Amount: {np.mean(all_investment_accuracy):.2f}%")
    print(f"  Tax Saving: {np.mean(all_tax_saving_accuracy):.2f}%")
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
    output_dir = Path("evaluation_results")
    output_dir.mkdir(exist_ok=True)
    
    output_file = output_dir / f"evaluation_results_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå: {output_file}")
    
    if save_logs:
        print(f"üìÇ Raw responses ‡πÅ‡∏•‡∏∞ logs ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà: {ai_service.log_dir}")
    
    print(f"\n{'='*80}")
    print("‚ú® EVALUATION COMPLETED!")
    print(f"{'='*80}\n")


async def quick_test(verbose: bool = True, save_logs: bool = True):
    """
    ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ
    """
    print("\nüß™ Quick Test - Evaluation Service with Raw Response\n")
    
    evaluator = EvaluationService()
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    reference = "‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF 200,000 ‡∏ö‡∏≤‡∏ó ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏´‡∏¢‡πà‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏µ‡πÑ‡∏î‡πâ 50,000 ‡∏ö‡∏≤‡∏ó ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 8% ‡∏ï‡πà‡∏≠‡∏õ‡∏µ"
    hypothesis = "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 200,000 ‡∏ö‡∏≤‡∏ó ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏©‡∏µ‡πÑ‡∏î‡πâ 50,000 ‡∏ö‡∏≤‡∏ó ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô 8% ‡∏ï‡πà‡∏≠‡∏õ‡∏µ"
    
    scores = evaluator.evaluate_single(reference, hypothesis, use_bertscore=False)
    evaluator.print_evaluation_report(scores)
    
    print("\n" + "="*80)
    print("‚ÑπÔ∏è  ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:")
    print("="*80)
    print("Quick test ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å OpenAI")
    print("‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏π raw response ‡∏à‡∏≤‡∏Å OpenAI ‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô:")
    print("  python backend/scripts/run_evaluation.py --mode full")
    print("="*80 + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Evaluate AI Tax Advisor')
    parser.add_argument(
        '--mode',
        choices=['full', 'quick'],
        default='quick',
        help='Evaluation mode: full (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î) or quick (‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏£‡πá‡∏ß)'
    )
    parser.add_argument(
        '--no-verbose',
        action='store_true',
        help='‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° debug'
    )
    parser.add_argument(
        '--no-save',
        action='store_true',
        help='‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å logs ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå'
    )
    
    args = parser.parse_args()
    
    verbose = not args.no_verbose
    save_logs = not args.no_save
    
    if args.mode == 'full':
        print("üöÄ Running FULL evaluation...")
        print(f"   - Verbose: {verbose}")
        print(f"   - Save logs: {save_logs}")
        asyncio.run(evaluate_ai_recommendations(verbose=verbose, save_logs=save_logs))
    else:
        print("‚ö° Running QUICK test...")
        asyncio.run(quick_test(verbose=verbose, save_logs=save_logs))