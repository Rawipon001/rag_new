"""
Evaluation Runner - Fixed Version (‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô)
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
"""

import sys
import os
import asyncio
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path
sys.path.insert(0, str(Path(__file__).parent.parent))

print("üîß Initializing...")

# Import evaluation modules
try:
    from app.services.evaluation_service import EvaluationService
    print("‚úÖ EvaluationService loaded")
except Exception as e:
    print(f"‚ùå Failed to import EvaluationService: {e}")
    print("\nüîß FIX: Install dependencies:")
    print("   pip install rouge-score nltk pythainlp")
    sys.exit(1)

try:
    from app.services.evaluation_test_data import EvaluationTestData
    print("‚úÖ EvaluationTestData loaded")
except Exception as e:
    print(f"‚ùå Failed to import EvaluationTestData: {e}")
    sys.exit(1)

try:
    from app.services.ai_service_for_evaluation import AIServiceForEvaluation
    print("‚úÖ AIServiceForEvaluation loaded")
except Exception as e:
    print(f"‚ùå Failed to import AIServiceForEvaluation: {e}")
    sys.exit(1)

# Import ‡∏£‡∏∞‡∏ö‡∏ö‡∏´‡∏•‡∏±‡∏Å
try:
    from app.services.rag_service import RAGService
    from app.services.tax_calculator import tax_calculator_service
    from app.models import TaxCalculationRequest
    print("‚úÖ Core services loaded")
except Exception as e:
    print(f"‚ùå Failed to import core services: {e}")
    sys.exit(1)

# NLTK data check
try:
    import nltk
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("üì• Downloading NLTK punkt data...")
        nltk.download('punkt', quiet=True)
        print("‚úÖ NLTK data ready")
except Exception as e:
    print(f"‚ö†Ô∏è  NLTK warning: {e}")


# ANSI Colors
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'


class EvaluationRunner:
    """
    Runner ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evaluation - Fixed Version
    """
    
    def __init__(
        self, 
        verbose: bool = True, 
        save_logs: bool = True,
        use_bertscore: bool = False
    ):
        self.verbose = verbose
        self.save_logs = save_logs
        self.use_bertscore = use_bertscore
        
        print("\nüöÄ Initializing Evaluation Runner...")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á services
        self.evaluator = EvaluationService()
        self.ai_service = AIServiceForEvaluation(verbose=verbose, save_to_file=save_logs)
        
        try:
            self.rag_service = RAGService()
            print("‚úÖ RAG Service initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è  RAG Service not available: {e}")
            self.rag_service = None
        
        # üìÅ ‡πÅ‡∏¢‡∏Å‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
        self.base_dir = Path("evaluation_output")
        self.logs_dir = self.base_dir / "logs"
        self.results_dir = self.base_dir / "results"
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå
        self.base_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
        print(f"üìÅ Output: {self.base_dir.absolute()}")
        
        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó ai_service ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs
        if hasattr(self.ai_service, 'log_dir'):
            self.ai_service.log_dir = self.logs_dir
        
        self.print_header()
    
    def print_header(self):
        """‡πÅ‡∏™‡∏î‡∏á Header ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
        print("\n" + "="*80)
        print(f"{Colors.BOLD}{Colors.CYAN}üöÄ AI TAX ADVISOR - EVALUATION SYSTEM{Colors.END}")
        print("="*80)
        print(f"  üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  üîß Mode: {'Verbose' if self.verbose else 'Silent'}")
        print(f"  üíæ Save logs: {'Yes' if self.save_logs else 'No'}")
        print(f"  ü§ñ BERTScore: {'Enabled' if self.use_bertscore else 'Disabled'}")
        print(f"\n  üìÅ Output directories:")
        print(f"     Logs:    {self.logs_dir}")
        print(f"     Results: {self.results_dir}")
        print("="*80 + "\n")
    
    def print_progress(self, current: int, total: int, message: str = ""):
        """‡πÅ‡∏™‡∏î‡∏á Progress"""
        percentage = (current / total) * 100
        filled = int((current / total) * 40)
        bar = '‚ñà' * filled + '‚ñë' * (40 - filled)
        
        print(f"\r  Progress: {bar} {percentage:.0f}% - {message}", end='', flush=True)
        
        if current == total:
            print()  # New line when complete
    
    async def run_single_test_case(
        self,
        test_case: Dict[str, Any],
        test_case_id: int
    ) -> Dict[str, Any]:
        """‡∏£‡∏±‡∏ô 1 test case"""
        
        test_name = test_case.get('name', f'Test Case {test_case_id}')
        
        print("\n" + "="*80)
        print(f"{Colors.BOLD}{Colors.BLUE}üìã TEST CASE {test_case_id}: {test_name}{Colors.END}")
        print("="*80)
        
        description = test_case.get('description', 'N/A')
        if description:
            print(f"  {Colors.CYAN}{description}{Colors.END}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á request
        request_data = test_case['input']
        request = TaxCalculationRequest(**request_data)
        
        print(f"\n  üí∞ ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ: {Colors.YELLOW}{request.gross_income:,}{Colors.END} ‡∏ö‡∏≤‡∏ó")
        print(f"  üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: {Colors.YELLOW}{request.risk_tolerance}{Colors.END}")
        
        # Step 1: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏©‡∏µ
        print(f"\n  {Colors.CYAN}[1/4]{Colors.END} ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏©‡∏µ...", end='', flush=True)
        try:
            tax_result = tax_calculator_service.calculate_tax(request)
            print(f" {Colors.GREEN}‚úì{Colors.END}")
            print(f"     ‚îî‚îÄ ‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏™‡∏∏‡∏ó‡∏ò‡∏¥: {tax_result.taxable_income:,} ‡∏ö‡∏≤‡∏ó")
            print(f"     ‚îî‚îÄ ‡∏†‡∏≤‡∏©‡∏µ: {tax_result.tax_amount:,} ‡∏ö‡∏≤‡∏ó ({tax_result.effective_tax_rate:.2f}%)")
        except Exception as e:
            print(f" {Colors.RED}‚úó{Colors.END}")
            print(f"     Error: {e}")
            return {}
        
        # Step 2: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG
        print(f"  {Colors.CYAN}[2/4]{Colors.END} ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG...", end='', flush=True)
        query = f"‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ {request.gross_income} ‡∏ö‡∏≤‡∏ó ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á {request.risk_tolerance}"
        
        context = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG"
        if self.rag_service:
            try:
                retrieved_docs = await self.rag_service.retrieve_relevant_documents(query, k=5)
                context_parts = []
                for doc in retrieved_docs:
                    if hasattr(doc, 'page_content'):
                        context_parts.append(doc.page_content)
                    elif hasattr(doc, 'content'):
                        context_parts.append(doc.content)
                    elif isinstance(doc, str):
                        context_parts.append(doc)
                
                context = "\n\n".join(context_parts) if context_parts else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG"
                print(f" {Colors.GREEN}‚úì{Colors.END} ({len(context)} chars)")
            except Exception as e:
                print(f" {Colors.YELLOW}‚ö†{Colors.END}")
                if self.verbose:
                    print(f"     Warning: {e}")
        else:
            print(f" {Colors.YELLOW}‚ö†{Colors.END} (Not available)")
        
        expected_plans = test_case.get('expected_plans', {})
        # Step 3: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI
        print(f"  {Colors.CYAN}[3/4]{Colors.END} ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å OpenAI...", end='', flush=True)
        try:
            ai_response, raw_response = await self.ai_service.generate_recommendations(
    request, tax_result, context, expected_plans, test_case_id
)
            print(f" {Colors.GREEN}‚úì{Colors.END} ({len(ai_response.get('plans', []))} plans)")
        except Exception as e:
            print(f" {Colors.RED}‚úó{Colors.END}")
            print(f"     Error: {e}")
            return {}

        # ‚ú® =================================================================
        # ‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏Ñ‡πâ‡∏î‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Evaluation Script ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
        # ‚ú® =================================================================
        print(f"  {Colors.CYAN}[Post-processing]{Colors.END} ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç...", end='', flush=True)

        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î tiers ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö AI service ‡πÅ‡∏•‡∏∞ main.py)
        gross = tax_result.gross_income
        if gross < 600000:
            tiers = [40000, 60000, 80000]
        elif gross < 1000000:
            tiers = [60000, 100000, 150000]
        elif gross < 1500000:
            tiers = [200000, 350000, 500000]
        elif gross < 2000000:
            tiers = [300000, 500000, 800000]
        elif gross < 3000000:
            tiers = [500000, 800000, 1200000]
        else:
            tiers = [800000, 1200000, 1800000]

        # üîß FIX: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì tax saving ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏µ
        # Tax Saving = ‡∏†‡∏≤‡∏©‡∏µ‡∏ó‡∏µ‡πà‡∏•‡∏î‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô
        # = (‡∏†‡∏≤‡∏©‡∏µ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏•‡∏á‡∏ó‡∏∏‡∏ô) - (‡∏†‡∏≤‡∏©‡∏µ‡∏ñ‡πâ‡∏≤‡∏•‡∏á‡∏ó‡∏∏‡∏ô)
        # = Investment √ó Marginal Rate ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô

        for idx, plan in enumerate(ai_response.get("plans", [])):
            # üéØ ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ total_investment ‡∏ï‡∏≤‡∏° tier (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å AI)
            if idx < len(tiers):
                total_investment = tiers[idx]
                plan["total_investment"] = total_investment  # Override AI's value
            else:
                total_investment = plan.get("total_investment", 0)

            # üîß ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì total tax saving ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡πà‡∏≠‡∏ô
            # ‡πÉ‡∏ä‡πâ total_investment ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ marginal rate ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            taxable_without_total_investment = tax_result.taxable_income + total_investment
            marginal_rate_for_total = tax_calculator_service.get_marginal_tax_rate(
                taxable_without_total_investment
            )
            calculated_total_tax_saving = int(total_investment * (marginal_rate_for_total / 100))

            # ‡πÅ‡∏à‡∏Å‡∏à‡πà‡∏≤‡∏¢ tax saving ‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞ allocation ‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
            for alloc in plan.get("allocations", []):
                percentage = alloc.get("percentage", 0)

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì investment_amount ‡∏à‡∏≤‡∏Å percentage
                investment_amount = int((percentage / 100) * total_investment)
                alloc["investment_amount"] = investment_amount

                # ‡πÅ‡∏à‡∏Å‡∏à‡πà‡∏≤‡∏¢ tax_saving ‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô percentage
                tax_saving = int((percentage / 100) * calculated_total_tax_saving)
                alloc["tax_saving"] = tax_saving

            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï total_tax_saving ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ú‡∏ô‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
            plan["total_tax_saving"] = calculated_total_tax_saving

        print(f" {Colors.GREEN}‚úì{Colors.END}")
        # ‚ú® =================================================================

        # Step 4: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ (Legal Compliance Check) üÜï
        print(f"  {Colors.CYAN}[4/5]{Colors.END} ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢...", end='', flush=True)

        legal_checks = []
        has_legal_violations = False

        for plan_idx, plan in enumerate(ai_response.get("plans", [])):
            legal_result = self.evaluator.validate_legal_compliance(
                plan=plan,
                gross_income=tax_result.gross_income,
                verbose=False  # ‡πÑ‡∏°‡πà print ‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞ print ‡∏£‡∏ß‡∏°‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Ç‡πâ‡∏≤‡∏á‡∏•‡πà‡∏≤‡∏á
            )
            legal_checks.append(legal_result)

            if not legal_result["is_legal"]:
                has_legal_violations = True

        if has_legal_violations:
            print(f" {Colors.RED}‚úó{Colors.END} (Found violations)")
        else:
            print(f" {Colors.GREEN}‚úì{Colors.END} (All legal)")

        # Step 5: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•
        print(f"  {Colors.CYAN}[5/5]{Colors.END} ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•...", end='', flush=True)


        if not expected_plans:
            print(f" {Colors.YELLOW}‚ö†{Colors.END} (No expected plans - skipping evaluation)")
            return {
                'test_case_id': test_case_id,
                'test_case_name': test_name,
                'status': 'no_expected_plans',
                'ai_response': ai_response,
                'legal_checks': legal_checks
            }

        try:
            evaluation_results = self.evaluator.evaluate_complete_response(
                expected_plans,
                ai_response,
                use_bertscore=self.use_bertscore
            )

            # üÜï ‡πÄ‡∏û‡∏¥‡πà‡∏° Legal Compliance Score ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô evaluation_results
            evaluation_results['legal_compliance'] = {
                'checks': legal_checks,
                'has_violations': has_legal_violations,
                'overall_score': 100 if not has_legal_violations else 0
            }

            # üÜï ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ violations ‡πÉ‡∏´‡πâ‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Numerical Accuracy ‡πÄ‡∏õ‡πá‡∏ô 0
            if has_legal_violations:
                # ‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ç‡∏≠‡∏á plans ‡∏ó‡∏µ‡πà‡∏°‡∏µ violations
                for plan_idx, legal_check in enumerate(legal_checks):
                    if not legal_check["is_legal"]:
                        plan_key = f"plan_{plan_idx + 1}"
                        if plan_key in evaluation_results:
                            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å numerical accuracy ‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏ß‡πâ
                            original_numerical = evaluation_results[plan_key].get("numerical_accuracy", {})
                            evaluation_results[plan_key]["numerical_accuracy_before_legal_check"] = original_numerical

                            # ‡∏•‡∏î‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ú‡∏¥‡∏î‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
                            evaluation_results[plan_key]["numerical_accuracy"] = {
                                "total_investment_match": 0.0,
                                "total_tax_saving_match": 0.0,
                                "overall_score": 0.0,
                                "reason": "FAILED - Legal violations detected"
                            }
                            evaluation_results[plan_key]["legal_check"] = legal_check

            print(f" {Colors.GREEN}‚úì{Colors.END}")
        except Exception as e:
            print(f" {Colors.RED}‚úó{Colors.END}")
            print(f"     Error: {e}")
            evaluation_results = {'error': str(e), 'legal_checks': legal_checks}
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        if 'error' not in evaluation_results:
            self.evaluator.print_evaluation_report(
                evaluation_results,
                test_case_name=test_name,
                save_to_file=self.save_logs,
                output_dir=self.results_dir
            )
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        result = {
            'test_case_id': test_case_id,
            'test_case_name': test_name,
            'input': request_data,
            'tax_result': {
                'gross_income': tax_result.gross_income,
                'taxable_income': tax_result.taxable_income,
                'tax_amount': tax_result.tax_amount,
                'effective_tax_rate': tax_result.effective_tax_rate
            },
            'ai_response': ai_response,
            'expected_plans': expected_plans,
            'evaluation_results': evaluation_results,
            'raw_response_preview': raw_response[:300] if raw_response else ""
        }
        
        return result
    
    async def run_all_test_cases(self) -> List[Dict[str, Any]]:
        """‡∏£‡∏±‡∏ô‡∏ó‡∏∏‡∏Å test cases"""
        
        test_cases = EvaluationTestData.get_all_test_cases()
        all_results = []
        
        print(f"\n{Colors.BOLD}üß™ RUNNING {len(test_cases)} TEST CASES{Colors.END}")
        print("="*80 + "\n")
        
        for i, test_case in enumerate(test_cases, 1):
            try:
                result = await self.run_single_test_case(test_case, i)
                if result:
                    all_results.append(result)

                # ‡πÅ‡∏™‡∏î‡∏á progress
                self.print_progress(i, len(test_cases), f"Completed {i}/{len(test_cases)}")

                # üîÑ Rate limiting: Add delay between test cases to prevent overwhelming API
                if i < len(test_cases):  # ‡πÑ‡∏°‡πà delay ‡∏´‡∏•‡∏±‡∏á test case ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                    if self.verbose:
                        print(f"\n‚è±Ô∏è  Rate limiting: Waiting 1.5s before next test case...")
                    await asyncio.sleep(1.5)

            except Exception as e:
                print(f"\n{Colors.RED}‚ùå Error in test case {i}: {e}{Colors.END}")
                if self.verbose:
                    import traceback
                    traceback.print_exc()
        
        return all_results
    
    def save_final_results(
        self,
        all_results: List[Dict[str, Any]],
        summary: Dict[str, Any]
    ):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        print(f"\n{Colors.BOLD}üíæ SAVING RESULTS{Colors.END}")
        print("‚îÄ"*80)
        
        # 1. Detailed Results
        detailed_file = self.results_dir / f"detailed_results_{timestamp}.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        print(f"  {Colors.GREEN}‚úì{Colors.END} Detailed results: {detailed_file.name}")
        
        # 2. Summary
        summary_file = self.results_dir / f"summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"  {Colors.GREEN}‚úì{Colors.END} Summary: {summary_file.name}")
        
        # 3. Human-readable Report
        report_file = self.results_dir / f"report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("AI TAX ADVISOR - EVALUATION REPORT\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"Total Test Cases: {summary.get('total_test_cases', 0)}\n\n")

            # Multi-Level Metrics (PRIMARY - MOST IMPORTANT)
            if 'multi_level_metrics' in summary and summary['multi_level_metrics']:
                f.write("="*80 + "\n")
                f.write("DESCRIPTION TEXT MATCHING (Primary Metric)\n")
                f.write("="*80 + "\n")
                f.write("These metrics show how well the AI matched the EXACT description text\n")
                f.write("="*80 + "\n\n")

                ml = summary['multi_level_metrics']

                # Show key metrics
                if 'avg_desc_bleu4' in ml:
                    val = ml['avg_desc_bleu4']
                    status = "PERFECT ‚úì" if val >= 0.95 else "GOOD ‚úì" if val >= 0.80 else "NEEDS IMPROVEMENT ‚úó"
                    f.write(f"  BLEU-4 (Description)      : {val:.4f} ({val*100:.1f}%) - {status}\n")

                if 'avg_desc_bertscore_f1' in ml:
                    val = ml['avg_desc_bertscore_f1']
                    status = "PERFECT ‚úì" if val >= 0.95 else "GOOD ‚úì" if val >= 0.80 else "NEEDS IMPROVEMENT ‚úó"
                    f.write(f"  BERTScore (Description)   : {val:.4f} ({val*100:.1f}%) - {status}\n")

                if 'avg_desc_rouge1_f1' in ml:
                    val = ml['avg_desc_rouge1_f1']
                    note = " (May be 0 due to Thai tokenization)" if val == 0 else ""
                    f.write(f"  ROUGE-1 F1 (Description)  : {val:.4f}{note}\n")

                if 'avg_desc_rougeL_f1' in ml:
                    val = ml['avg_desc_rougeL_f1']
                    note = " (May be 0 due to Thai tokenization)" if val == 0 else ""
                    f.write(f"  ROUGE-L F1 (Description)  : {val:.4f}{note}\n")

                f.write("\n")
                f.write("Supporting Metrics:\n")
                f.write("-"*40 + "\n")

                if 'avg_keyword_coverage_ratio' in ml:
                    f.write(f"  Keyword Coverage          : {ml['avg_keyword_coverage_ratio']:.2%}\n")

                if 'avg_semantic_bertscore_f1' in ml:
                    f.write(f"  Semantic Similarity       : {ml['avg_semantic_bertscore_f1']:.4f}\n")

                if 'avg_keypoint_coverage_ratio' in ml:
                    f.write(f"  Key Points Coverage       : {ml['avg_keypoint_coverage_ratio']:.2%}\n")

                f.write("\n")

            # Numeric metrics
            if 'numeric_metrics' in summary and summary['numeric_metrics']:
                f.write("NUMERIC ACCURACY:\n")
                f.write("-"*40 + "\n")
                for key, value in summary['numeric_metrics'].items():
                    metric_name = key.replace('_', ' ').title()
                    f.write(f"  {metric_name:15} : {value:.2f}%\n")
                f.write("\n")

            # Legacy Text metrics (for reference)
            if 'text_metrics' in summary and summary['text_metrics']:
                f.write("\n")
                f.write("="*80 + "\n")
                f.write("LEGACY FULL-TEXT METRICS (Less Relevant - For Reference Only)\n")
                f.write("="*80 + "\n")
                f.write("Note: These compare ALL text including allocations (pros/cons) we don't control\n")
                f.write("      Focus on 'Description Text Matching' metrics above instead!\n")
                f.write("="*80 + "\n\n")
                for key, value in summary['text_metrics'].items():
                    metric_name = key.replace('avg_', '').upper()
                    f.write(f"  {metric_name:25} : {value:.4f}\n")
                f.write("\n")
            
            # Test case details
            f.write("\n" + "="*80 + "\n")
            f.write("DETAILED RESULTS BY TEST CASE\n")
            f.write("="*80 + "\n\n")
            
            for result in all_results:
                f.write(f"Test Case {result['test_case_id']}: {result['test_case_name']}\n")
                f.write("-"*80 + "\n")

                eval_res = result.get('evaluation_results', {})
                if 'overall_metrics' in eval_res:
                    overall = eval_res['overall_metrics']
                    f.write(f"  Plans: {overall.get('actual_plan_count', 0)}/{overall.get('expected_plan_count', 3)}\n")

                    if 'avg_numeric_accuracy' in overall:
                        f.write(f"  Numeric Accuracy: {overall['avg_numeric_accuracy']:.2f}%\n")

                    # Show multi-level metrics if available
                    if 'multi_level_metrics' in overall:
                        ml = overall['multi_level_metrics']
                        if 'avg_desc_bleu4' in ml:
                            val = ml['avg_desc_bleu4']
                            f.write(f"  Description BLEU-4: {val:.4f} ({val*100:.1f}%)\n")
                        if 'avg_desc_bertscore_f1' in ml:
                            val = ml['avg_desc_bertscore_f1']
                            f.write(f"  Description BERTScore: {val:.4f} ({val*100:.1f}%)\n")
                    # Fallback to legacy metrics
                    elif 'text_metrics' in overall:
                        text_m = overall['text_metrics']
                        if 'avg_rouge1_f1' in text_m:
                            f.write(f"  ROUGE-1 F1 (Legacy): {text_m['avg_rouge1_f1']:.4f}\n")
                        if 'avg_bleu4' in text_m:
                            f.write(f"  BLEU-4 (Legacy): {text_m['avg_bleu4']:.4f}\n")

                f.write("\n")
        
        print(f"  {Colors.GREEN}‚úì{Colors.END} Report: {report_file.name}")
        print()


async def quick_test():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÜ"""
    
    print(f"\n{Colors.CYAN}üß™ Quick Test - Evaluation Service{Colors.END}\n")
    
    evaluator = EvaluationService()
    
    expected_plan = {
        "plan_id": "1",
        "plan_name": "‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 1 - ‡πÄ‡∏ô‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô",
        "description": "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢",
        "total_investment": 150000,
        "total_tax_saving": 15000,
        "allocations": [
            {
                "category": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
                "investment_amount": 50000,
                "percentage": 33.33,
                "tax_saving": 5000,
                "risk_level": "low"
            }
        ]
    }
    
    ai_plan = {
        "plan_id": "1",
        "plan_name": "‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 1 - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢",
        "description": "‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô",
        "total_investment": 145000,
        "total_tax_saving": 14500,
        "allocations": [
            {
                "category": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
                "investment_amount": 48000,
                "percentage": 33.10,
                "tax_saving": 4800,
                "risk_level": "low"
            }
        ]
    }
    
    print("Evaluating sample plan...")
    results = evaluator.evaluate_plan(expected_plan, ai_plan, use_bertscore=False)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
    evaluator.print_evaluation_report(
        {'plan_metrics': {'plan_1': results}},
        test_case_name="Quick Test Sample"
    )


async def main():
    """Main function"""
    
    parser = argparse.ArgumentParser(
        description='AI Tax Advisor - Enhanced Evaluation (Fixed)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_evaluation_complete.py --mode quick
  python run_evaluation_complete.py --mode full
  python run_evaluation_complete.py --mode full --test-case 1
  python run_evaluation_complete.py --mode full --bertscore
        """
    )
    
    parser.add_argument('--mode', choices=['quick', 'full'], default='quick',
                       help='Evaluation mode')
    parser.add_argument('--test-case', type=int,
                       help='Run specific test case (1-20)')
    parser.add_argument('--bertscore', action='store_true',
                       help='Use BERTScore (slower)')
    parser.add_argument('--no-verbose', action='store_true',
                       help='Disable verbose logging')
    parser.add_argument('--no-save', action='store_true',
                       help='Disable saving logs')
    
    args = parser.parse_args()
    
    if args.mode == 'quick':
        await quick_test()
        return
    
    # Full evaluation
    runner = EvaluationRunner(
        verbose=not args.no_verbose,
        save_logs=not args.no_save,
        use_bertscore=args.bertscore
    )
    
    # ‡∏£‡∏±‡∏ô specific test case ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    if args.test_case:
        test_case = EvaluationTestData.get_test_case_by_id(args.test_case)
        if not test_case:
            print(f"{Colors.RED}‚ùå Test case {args.test_case} not found{Colors.END}")
            return
        result = await runner.run_single_test_case(test_case, args.test_case)
        all_results = [result] if result else []
    else:
        all_results = await runner.run_all_test_cases()
    
    if not all_results:
        print(f"\n{Colors.YELLOW}‚ö†Ô∏è  No results to summarize{Colors.END}\n")
        return
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á summary
    print(f"\n{Colors.BOLD}{Colors.HEADER}üìà GENERATING SUMMARY{Colors.END}")
    print("="*80 + "\n")
    
    evaluation_results = [r['evaluation_results'] for r in all_results 
                         if 'evaluation_results' in r and 'error' not in r['evaluation_results']]
    
    if evaluation_results:
        summary = runner.evaluator.generate_summary_statistics(evaluation_results)
        runner.evaluator.print_summary_report(summary)
        runner.save_final_results(all_results, summary)
    else:
        print(f"{Colors.YELLOW}‚ö†Ô∏è  No valid evaluation results to summarize{Colors.END}\n")
        summary = {'total_test_cases': len(all_results)}
        runner.save_final_results(all_results, summary)

    # üìä Print API Retry Statistics
    print(f"\n{Colors.BOLD}{Colors.HEADER}üìä API RELIABILITY STATISTICS{Colors.END}")
    print("="*80)
    runner.ai_service.print_retry_statistics()

    # Final summary
    print(f"\n{Colors.BOLD}{Colors.GREEN}‚úÖ EVALUATION COMPLETED!{Colors.END}")
    print("="*80)
    print(f"  Test cases: {len(all_results)}")
    print(f"  Results: {runner.results_dir}")
    print(f"  Logs: {runner.logs_dir}")
    
    if evaluation_results and 'numeric_metrics' in summary and 'avg_accuracy' in summary['numeric_metrics']:
        acc = summary['numeric_metrics']['avg_accuracy']
        print(f"  Average accuracy: {Colors.CYAN}{acc:.2f}%{Colors.END}")
    
    print("="*80 + "\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print(f"\n\n{Colors.YELLOW}‚ö†Ô∏è  Interrupted by user{Colors.END}\n")
    except Exception as e:
        print(f"\n\n{Colors.RED}‚ùå Fatal error: {e}{Colors.END}\n")
        import traceback
        traceback.print_exc()
        sys.exit(1)