"""
Evaluation Service - Enhanced Version
‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö
‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ROUGE, BLEU, BERTScore ‡πÄ‡∏ï‡πá‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
"""

from typing import Dict, List, Any, Tuple
import numpy as np
import json
from pathlib import Path
from datetime import datetime

# ROUGE
from rouge_score import rouge_scorer, tokenizers

# BLEU
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# BERTScore
try:
    from bert_score import score as bert_score
    BERTSCORE_AVAILABLE = True
except:
    BERTSCORE_AVAILABLE = False
    print("‚ö†Ô∏è  BERTScore not available. Install with: pip install bert-score")

# Thai tokenizer
try:
    from pythainlp.tokenize import word_tokenize
    THAI_TOKENIZER_AVAILABLE = True
except:
    THAI_TOKENIZER_AVAILABLE = False
    print("‚ö†Ô∏è  PyThaiNLP not available. Install with: pip install pythainlp")


class Colors:
    """ANSI Color codes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö terminal"""
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


class SimpleTokenizer(tokenizers.Tokenizer):
    """Custom tokenizer for pre-tokenized text (e.g., Thai text)"""
    def tokenize(self, text):
        return text.split()


class EvaluationService:
    """
    Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö AI Tax Advisor
    Enhanced Version - ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ROUGE, BLEU, BERTScore with Thai language support
    """

    def __init__(self):
        # ROUGE scorer with custom tokenizer for Thai support
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'],
            use_stemmer=False,
            tokenizer=SimpleTokenizer()
        )

        # BLEU smoothing
        self.smoothing = SmoothingFunction().method1

        status = f"{Colors.GREEN}‚úÖ Evaluation Service initialized{Colors.END}"
        status += f" {Colors.CYAN}(with Legal Compliance Check){Colors.END}"
        if BERTSCORE_AVAILABLE:
            status += f" (BERTScore: {Colors.GREEN}available{Colors.END})"
        else:
            status += f" (BERTScore: {Colors.YELLOW}not available{Colors.END})"

        print(status)

    def validate_legal_compliance(
        self,
        plan: Dict[str, Any],
        gross_income: int,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

        ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:
        1. ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç: ‚â§ min(15% of income, 200,000)
        2. RMF: ‚â§ min(30% of income, 500,000)
        3. ThaiESG/ThaiESGX: ‚â§ min(30% of income, 300,000)
        4. PVD: ‚â§ min(15% of income, 500,000)
        5. GPF: ‚â§ min(30% of income, 500,000)
        6. Life Insurance: ‚â§ 100,000
        7. Health Insurance: ‚â§ 25,000
        8. Combined Life + Health: ‚â§ 125,000

        Returns:
            {
                "is_legal": True/False,
                "violations": [...],
                "legal_compliance_score": 0-100,
                "details": {...}
            }
        """
        violations = []
        warnings = []

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ï‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
        limits = {
            "pension_insurance": min(int(gross_income * 0.15), 200000),
            "rmf": min(int(gross_income * 0.30), 500000),
            "thai_esg": min(int(gross_income * 0.30), 300000),
            "thai_esgx": min(int(gross_income * 0.30), 300000),
            "pvd": min(int(gross_income * 0.15), 500000),
            "gpf": min(int(gross_income * 0.30), 500000),
            "life_insurance": 100000,
            "health_insurance": 25000,
            "combined_insurance": 125000
        }

        total_investment = plan.get("total_investment", 0)
        allocations = plan.get("allocations", [])

        # ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏¢‡∏≠‡∏î‡∏£‡∏ß‡∏°
        totals = {
            "pension_insurance": 0,
            "rmf": 0,
            "thai_esg": 0,
            "life_insurance": 0,
            "health_insurance": 0
        }

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
        for idx, allocation in enumerate(allocations):
            category = allocation.get("category", "")
            category_lower = category.lower()
            percentage = allocation.get("percentage", 0)
            amount = allocation.get("investment_amount")

            # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ investment_amount ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å percentage
            if amount is None and total_investment > 0:
                amount = int(total_investment * percentage / 100)

            if amount is None or amount <= 0:
                continue

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç
            if "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç" in category or "‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç" in category_lower:
                totals["pension_insurance"] += amount
                if amount > limits["pension_insurance"]:
                    violations.append({
                        "category": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç",
                        "allocation_index": idx,
                        "recommended_amount": amount,
                        "legal_max": limits["pension_insurance"],
                        "excess": amount - limits["pension_insurance"],
                        "violation_percentage": ((amount - limits["pension_insurance"]) / limits["pension_insurance"]) * 100,
                        "reason": f"‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î 15% ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ ({int(gross_income * 0.15):,}) ‡∏´‡∏£‡∏∑‡∏≠ 200,000 ‡∏ö‡∏≤‡∏ó",
                        "law_reference": "tax_deductions_update280168.pdf, Page 2, Item 13"
                    })

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö RMF
            if "rmf" in category_lower:
                totals["rmf"] += amount
                if amount > limits["rmf"]:
                    violations.append({
                        "category": "RMF",
                        "allocation_index": idx,
                        "recommended_amount": amount,
                        "legal_max": limits["rmf"],
                        "excess": amount - limits["rmf"],
                        "violation_percentage": ((amount - limits["rmf"]) / limits["rmf"]) * 100,
                        "reason": f"‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î 30% ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ ({int(gross_income * 0.30):,}) ‡∏´‡∏£‡∏∑‡∏≠ 500,000 ‡∏ö‡∏≤‡∏ó",
                        "law_reference": "tax_deductions_update280168.pdf, Page 1, Item 12"
                    })

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ThaiESG/ThaiESGX
            if "thaiesg" in category_lower or "esg" in category_lower:
                totals["thai_esg"] += amount
                if amount > limits["thai_esg"]:
                    violations.append({
                        "category": "ThaiESG/ThaiESGX",
                        "allocation_index": idx,
                        "recommended_amount": amount,
                        "legal_max": limits["thai_esg"],
                        "excess": amount - limits["thai_esg"],
                        "violation_percentage": ((amount - limits["thai_esg"]) / limits["thai_esg"]) * 100,
                        "reason": f"‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î 30% ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ ({int(gross_income * 0.30):,}) ‡∏´‡∏£‡∏∑‡∏≠ 300,000 ‡∏ö‡∏≤‡∏ó",
                        "law_reference": "tax_deductions_update280168.pdf, Page 2, Item 21"
                    })

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï
            if "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï" in category and "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û" not in category and "‡∏ö‡∏≥‡∏ô‡∏≤‡∏ç" not in category:
                totals["life_insurance"] += amount
                if amount > limits["life_insurance"]:
                    violations.append({
                        "category": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï",
                        "allocation_index": idx,
                        "recommended_amount": amount,
                        "legal_max": limits["life_insurance"],
                        "excess": amount - limits["life_insurance"],
                        "violation_percentage": ((amount - limits["life_insurance"]) / limits["life_insurance"]) * 100,
                        "reason": "‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î 100,000 ‡∏ö‡∏≤‡∏ó",
                        "law_reference": "tax_deductions_update280168.pdf, Page 1, Item 8"
                    })

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö ‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û
            if "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û" in category or ("‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û" in category and "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï" not in category):
                totals["health_insurance"] += amount
                if amount > limits["health_insurance"]:
                    violations.append({
                        "category": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
                        "allocation_index": idx,
                        "recommended_amount": amount,
                        "legal_max": limits["health_insurance"],
                        "excess": amount - limits["health_insurance"],
                        "violation_percentage": ((amount - limits["health_insurance"]) / limits["health_insurance"]) * 100,
                        "reason": "‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î 25,000 ‡∏ö‡∏≤‡∏ó",
                        "law_reference": "tax_deductions_update280168.pdf, Page 1, Item 9"
                    })

            # Combined (‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û)
            if "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï" in category and "‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û" in category:
                estimated_life = int(amount * 0.8)
                estimated_health = int(amount * 0.2)
                totals["life_insurance"] += estimated_life
                totals["health_insurance"] += estimated_health

        # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏£‡∏ß‡∏° Life + Health Insurance
        combined_insurance = totals["life_insurance"] + totals["health_insurance"]
        if combined_insurance > limits["combined_insurance"]:
            warnings.append({
                "category": "‡∏£‡∏ß‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï + ‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û",
                "total_amount": combined_insurance,
                "legal_max": limits["combined_insurance"],
                "excess": combined_insurance - limits["combined_insurance"],
                "reason": "‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 125,000 ‡∏ö‡∏≤‡∏ó"
            })

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô Legal Compliance
        is_legal = len(violations) == 0
        legal_compliance_score = 100 if is_legal else 0

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ñ‡πâ‡∏≤ verbose
        if verbose and violations:
            print(f"\n{Colors.RED}üö® LEGAL VIOLATIONS DETECTED in Plan {plan.get('plan_id', '?')}{Colors.END}")
            for v in violations:
                print(f"   {Colors.RED}‚ùå{Colors.END} {v['category']}: {v['recommended_amount']:,} ‡∏ö‡∏≤‡∏ó")
                print(f"      Legal Max: {v['legal_max']:,} ‡∏ö‡∏≤‡∏ó")
                print(f"      Excess: {v['excess']:,} ‡∏ö‡∏≤‡∏ó ({v['violation_percentage']:.1f}% over)")
                print(f"      Reason: {v['reason']}")

        if verbose and warnings:
            print(f"\n{Colors.YELLOW}‚ö†Ô∏è  WARNINGS in Plan {plan.get('plan_id', '?')}{Colors.END}")
            for w in warnings:
                print(f"   {Colors.YELLOW}‚ö†Ô∏è {Colors.END} {w['category']}: {w['total_amount']:,} > {w['legal_max']:,}")

        return {
            "is_legal": is_legal,
            "violations": violations,
            "warnings": warnings,
            "legal_compliance_score": legal_compliance_score,
            "totals": totals,
            "limits": limits,
            "details": {
                "gross_income": gross_income,
                "total_violations": len(violations),
                "total_warnings": len(warnings)
            }
        }
    
    def tokenize_thai(self, text: str) -> List[str]:
        """Tokenize ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        if THAI_TOKENIZER_AVAILABLE:
            tokens = word_tokenize(text, engine='newmm')
            # Filter out whitespace tokens
            return [token for token in tokens if token.strip()]
        else:
            return text.split()
    
    def calculate_rouge(self, reference: str, hypothesis: str) -> Dict[str, float]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE scores - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏ß‡∏¢ word tokenization"""
        # Tokenize Thai text first
        ref_tokens = self.tokenize_thai(reference)
        hyp_tokens = self.tokenize_thai(hypothesis)

        # Join with spaces so ROUGE can process properly
        ref_text = ' '.join(ref_tokens)
        hyp_text = ' '.join(hyp_tokens)

        scores = self.rouge_scorer.score(ref_text, hyp_text)

        return {
            'rouge1_precision': scores['rouge1'].precision,
            'rouge1_recall': scores['rouge1'].recall,
            'rouge1_f1': scores['rouge1'].fmeasure,
            'rouge2_precision': scores['rouge2'].precision,
            'rouge2_recall': scores['rouge2'].recall,
            'rouge2_f1': scores['rouge2'].fmeasure,
            'rougeL_precision': scores['rougeL'].precision,
            'rougeL_recall': scores['rougeL'].recall,
            'rougeL_f1': scores['rougeL'].fmeasure,
        }
    
    def calculate_bleu(self, reference: str, hypothesis: str) -> Dict[str, float]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BLEU score"""
        ref_tokens = self.tokenize_thai(reference)
        hyp_tokens = self.tokenize_thai(hypothesis)
        
        bleu1 = sentence_bleu([ref_tokens], hyp_tokens, weights=(1, 0, 0, 0), smoothing_function=self.smoothing)
        bleu2 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=self.smoothing)
        bleu3 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=self.smoothing)
        bleu4 = sentence_bleu([ref_tokens], hyp_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=self.smoothing)
        
        return {
            'bleu1': bleu1,
            'bleu2': bleu2,
            'bleu3': bleu3,
            'bleu4': bleu4,
        }
    
    def calculate_keypoint_coverage(self, key_points: List[str], ai_full_text: str, use_bertscore: bool = False) -> Dict[str, Any]:
        """
        Calculate key points coverage - checks if each key point is semantically present in AI text

        Args:
            key_points: List of expected key points
            ai_full_text: Full AI generated text (description + plan details)
            use_bertscore: If True, use BERTScore for semantic matching; otherwise use substring matching

        Returns:
            Dictionary with coverage metrics
        """
        if not key_points:
            return {'coverage_ratio': 0.0, 'covered_points': 0, 'total_points': 0}

        covered_points = 0
        point_scores = []

        for key_point in key_points:
            if use_bertscore and BERTSCORE_AVAILABLE:
                # Semantic matching using BERTScore
                try:
                    _, _, F1 = bert_score([ai_full_text], [key_point], lang='th', verbose=False)
                    score = F1.mean().item()
                    # Threshold: consider covered if BERTScore F1 > 0.7
                    is_covered = score > 0.7
                    point_scores.append(score)
                except:
                    # Fallback to substring matching
                    is_covered = key_point.lower() in ai_full_text.lower()
                    point_scores.append(1.0 if is_covered else 0.0)
            else:
                # Simple substring matching (faster but less accurate)
                is_covered = key_point.lower() in ai_full_text.lower()
                point_scores.append(1.0 if is_covered else 0.0)

            if is_covered:
                covered_points += 1

        coverage_ratio = covered_points / len(key_points) if key_points else 0.0
        avg_score = np.mean(point_scores) if point_scores else 0.0

        return {
            'coverage_ratio': coverage_ratio,
            'covered_points': covered_points,
            'total_points': len(key_points),
            'average_score': avg_score,
            'individual_scores': point_scores
        }

    def calculate_bertscore(self, reference: str, hypothesis: str) -> Dict[str, float]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore"""
        if not BERTSCORE_AVAILABLE:
            return {}

        try:
            P, R, F1 = bert_score([hypothesis], [reference], lang='th', verbose=False)
            return {
                'bertscore_precision': P.mean().item(),
                'bertscore_recall': R.mean().item(),
                'bertscore_f1': F1.mean().item()
            }
        except Exception as e:
            print(f"  {Colors.YELLOW}‚ö†Ô∏è  BERTScore error: {e}{Colors.END}")
            return {}
    
    def calculate_numeric_accuracy(self, expected_value: float, actual_value: float, tolerance: float = 0.1) -> Tuple[float, bool]:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç"""
        if expected_value == 0:
            return (100.0, actual_value == 0)
        
        error_percentage = abs(expected_value - actual_value) / expected_value
        accuracy = max(0, (1 - error_percentage) * 100)
        is_within_tolerance = error_percentage <= tolerance
        
        return (accuracy, is_within_tolerance)
    
    def get_score_color(self, score: float, metric_type: str = 'general') -> str:
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô"""
        if metric_type == 'accuracy':
            if score >= 90:
                return Colors.GREEN
            elif score >= 75:
                return Colors.YELLOW
            else:
                return Colors.RED
        else:  # text metrics
            if score >= 0.4:
                return Colors.GREEN
            elif score >= 0.25:
                return Colors.YELLOW
            else:
                return Colors.RED
    
    def get_score_emoji(self, score: float, metric_type: str = 'general') -> str:
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å emoji ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô"""
        if metric_type == 'accuracy':
            if score >= 90:
                return "üéØ"
            elif score >= 75:
                return "‚úÖ"
            else:
                return "‚ö†Ô∏è"
        else:  # text metrics
            if score >= 0.4:
                return "üéØ"
            elif score >= 0.25:
                return "‚úÖ"
            else:
                return "‚ö†Ô∏è"
    
    def print_progress_bar(self, value: float, max_value: float = 100, width: int = 30):
        """‡πÅ‡∏™‡∏î‡∏á progress bar"""
        percentage = (value / max_value) * 100
        filled = int((value / max_value) * width)
        bar = '‚ñà' * filled + '‚ñë' * (width - filled)
        
        color = self.get_score_color(percentage, 'accuracy')
        emoji = self.get_score_emoji(percentage, 'accuracy')
        
        return f"{emoji} {color}{bar}{Colors.END} {percentage:.1f}%"
    
    def evaluate_plan(self, expected_plan: Dict[str, Any], ai_plan: Dict[str, Any], use_bertscore: bool = False) -> Dict[str, Any]:
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 1 ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÅ‡∏ö‡∏ö Multi-Level Evaluation

        Level 1: Description Similarity (ROUGE, BLEU, BERTScore)
        Level 2: Keyword Coverage
        Level 3: Semantic Similarity (BERTScore)
        Level 4: Key Points Coverage
        """
        results = {
            'text_metrics': {},
            'numeric_metrics': {},
            'structural_metrics': {},
            'multi_level_metrics': {}  # NEW: Separate multi-level scores
        }

        # Extract expected text structure
        expected_text_data = expected_plan.get('expected_text', {})

        # Get AI generated text
        ai_description = ai_plan.get('description', '')
        ai_plan_name = ai_plan.get('plan_name', '')

        # Construct full AI text for key points coverage
        ai_allocations_text = ' '.join([
            f"{alloc.get('category', '')} {' '.join(alloc.get('pros', []))} {' '.join(alloc.get('cons', []))}"
            for alloc in ai_plan.get('allocations', [])
        ])
        ai_full_text = f"{ai_description} {ai_plan_name} {ai_allocations_text}"

        if expected_text_data:
            # ========================================
            # MULTI-LEVEL EVALUATION (Option 3)
            # ========================================

            expected_description = expected_text_data.get('description', '')
            expected_keywords = expected_text_data.get('keywords', [])
            expected_key_points = expected_text_data.get('key_points', [])

            # -----------------------------------------
            # LEVEL 1: Description-Only Similarity
            # -----------------------------------------
            if expected_description.strip() and ai_description.strip():
                # ROUGE (on description only)
                rouge_scores = self.calculate_rouge(expected_description, ai_description)
                for key, value in rouge_scores.items():
                    results['multi_level_metrics'][f'desc_{key}'] = value

                # BLEU (on description only)
                bleu_scores = self.calculate_bleu(expected_description, ai_description)
                for key, value in bleu_scores.items():
                    results['multi_level_metrics'][f'desc_{key}'] = value

                # BERTScore (on description only)
                if use_bertscore:
                    bert_scores = self.calculate_bertscore(expected_description, ai_description)
                    for key, value in bert_scores.items():
                        results['multi_level_metrics'][f'desc_{key}'] = value

            # -----------------------------------------
            # LEVEL 2: Keyword Coverage
            # -----------------------------------------
            if expected_keywords:
                keyword_match_count = sum(1 for kw in expected_keywords if kw.lower() in ai_full_text.lower())
                keyword_match_ratio = keyword_match_count / len(expected_keywords)
                results['multi_level_metrics']['keyword_coverage_ratio'] = keyword_match_ratio
                results['multi_level_metrics']['keywords_matched'] = keyword_match_count
                results['multi_level_metrics']['keywords_total'] = len(expected_keywords)

            # -----------------------------------------
            # LEVEL 3: Semantic Similarity (Full Text)
            # -----------------------------------------
            if use_bertscore and ai_full_text.strip():
                expected_full = f"{expected_description} {' '.join(expected_keywords)} {' '.join(expected_key_points)}"
                semantic_scores = self.calculate_bertscore(expected_full, ai_full_text)
                for key, value in semantic_scores.items():
                    results['multi_level_metrics'][f'semantic_{key}'] = value

            # -----------------------------------------
            # LEVEL 4: Key Points Coverage
            # -----------------------------------------
            if expected_key_points:
                keypoint_coverage = self.calculate_keypoint_coverage(
                    expected_key_points,
                    ai_full_text,
                    use_bertscore=False  # Use substring matching for speed
                )
                results['multi_level_metrics']['keypoint_coverage_ratio'] = keypoint_coverage['coverage_ratio']
                results['multi_level_metrics']['keypoints_covered'] = keypoint_coverage['covered_points']
                results['multi_level_metrics']['keypoints_total'] = keypoint_coverage['total_points']
                results['multi_level_metrics']['keypoint_avg_score'] = keypoint_coverage['average_score']

            # ========================================
            # LEGACY METRICS (for backward compatibility)
            # ========================================
            # Get expected plan_name
            expected_plan_name = expected_text_data.get('plan_name', '')

            # Build expected allocations text
            expected_allocs_text = ""
            if 'expected_allocations' in expected_text_data:
                for alloc in expected_text_data['expected_allocations']:
                    cat = alloc.get('category', '')
                    pros = ' '.join(alloc.get('pros', []))
                    cons = ' '.join(alloc.get('cons', []))
                    expected_allocs_text += f" {cat} {pros} {cons}"

            # Build AI allocations text
            ai_allocs_text = ""
            for alloc in ai_plan.get('allocations', []):
                cat = alloc.get('category', '')
                pros = ' '.join(alloc.get('pros', []))
                cons = ' '.join(alloc.get('cons', []))
                ai_allocs_text += f" {cat} {pros} {cons}"

            # Include plan_name and allocations in legacy comparison
            ai_text_legacy = f"{ai_description} {ai_plan_name} {ai_allocs_text}"
            expected_text_legacy = f"{expected_description} {expected_plan_name} {' '.join(expected_keywords)} {' '.join(expected_key_points)} {expected_allocs_text}"

            if expected_text_legacy.strip() and ai_text_legacy.strip():
                rouge_legacy = self.calculate_rouge(expected_text_legacy, ai_text_legacy)
                results['text_metrics'].update(rouge_legacy)

                bleu_legacy = self.calculate_bleu(expected_text_legacy, ai_text_legacy)
                results['text_metrics'].update(bleu_legacy)

                if use_bertscore:
                    bert_legacy = self.calculate_bertscore(expected_text_legacy, ai_text_legacy)
                    results['text_metrics'].update(bert_legacy)

                if expected_keywords:
                    keyword_match_count = sum(1 for kw in expected_keywords if kw.lower() in ai_text_legacy.lower())
                    keyword_match_ratio = keyword_match_count / len(expected_keywords)
                    results['text_metrics']['keyword_match_ratio'] = keyword_match_ratio

        else:
            # Legacy: fallback to old structure (no expected_text)
            expected_text = f"{expected_plan.get('description', '')} {expected_plan.get('plan_name', '')}"
            ai_text = f"{ai_description} {ai_plan_name}"

            if expected_text.strip() and ai_text.strip():
                rouge_scores = self.calculate_rouge(expected_text, ai_text)
                results['text_metrics'].update(rouge_scores)

                bleu_scores = self.calculate_bleu(expected_text, ai_text)
                results['text_metrics'].update(bleu_scores)

                if use_bertscore:
                    bert_scores = self.calculate_bertscore(expected_text, ai_text)
                    results['text_metrics'].update(bert_scores)
        
        # 2. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° allocation categories
        expected_allocs = expected_plan.get('allocations', [])
        ai_allocs = ai_plan.get('allocations', [])
        
        allocation_text_scores = []
        for exp_alloc, ai_alloc in zip(expected_allocs, ai_allocs[:len(expected_allocs)]):
            exp_cat = exp_alloc.get('category', '')
            ai_cat = ai_alloc.get('category', '')
            
            if exp_cat and ai_cat:
                rouge = self.calculate_rouge(exp_cat, ai_cat)
                allocation_text_scores.append(rouge.get('rouge1_f1', 0))
        
        if allocation_text_scores:
            results['text_metrics']['avg_allocation_text_similarity'] = np.mean(allocation_text_scores)
        
        # 3. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        numeric_fields = ['total_investment', 'total_tax_saving']
        for field in numeric_fields:
            expected_val = expected_plan.get(field, 0)
            ai_val = ai_plan.get(field, 0)
            
            if expected_val > 0:
                accuracy, within_tolerance = self.calculate_numeric_accuracy(expected_val, ai_val, tolerance=0.15)
                
                results['numeric_metrics'][field] = {
                    'expected': expected_val,
                    'actual': ai_val,
                    'accuracy': accuracy,
                    'within_tolerance': within_tolerance,
                    'error_percentage': abs(expected_val - ai_val) / expected_val * 100 if expected_val > 0 else 0
                }
        
        # 4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á
        results['structural_metrics'] = {
            'expected_allocation_count': len(expected_allocs),
            'actual_allocation_count': len(ai_allocs),
            'has_correct_structure': 'allocations' in ai_plan and len(ai_allocs) > 0
        }
        
        # 5. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô allocations
        allocation_scores = []
        for i, (exp_alloc, ai_alloc) in enumerate(zip(expected_allocs, ai_allocs[:len(expected_allocs)])):
            alloc_score = self.evaluate_allocation(exp_alloc, ai_alloc)
            allocation_scores.append(alloc_score)
        
        if allocation_scores:
            results['allocation_metrics'] = {
                'individual_scores': allocation_scores,
                'average_investment_accuracy': np.mean([s.get('investment_accuracy', 0) for s in allocation_scores]),
                'average_tax_saving_accuracy': np.mean([s.get('tax_saving_accuracy', 0) for s in allocation_scores])
            }
        
        return results
    
    def evaluate_allocation(self, expected_alloc: Dict[str, Any], ai_alloc: Dict[str, Any]) -> Dict[str, float]:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô 1 allocation item"""
        results = {}
        
        # Investment amount accuracy
        exp_inv = expected_alloc.get('investment_amount', 0)
        ai_inv = ai_alloc.get('investment_amount', 0)
        if exp_inv > 0:
            inv_accuracy, _ = self.calculate_numeric_accuracy(exp_inv, ai_inv, tolerance=0.2)
            results['investment_accuracy'] = inv_accuracy
        
        # Tax saving accuracy
        exp_tax = expected_alloc.get('tax_saving', 0)
        ai_tax = ai_alloc.get('tax_saving', 0)
        if exp_tax > 0:
            tax_accuracy, _ = self.calculate_numeric_accuracy(exp_tax, ai_tax, tolerance=0.2)
            results['tax_saving_accuracy'] = tax_accuracy
        
        # Category match
        results['category_match'] = expected_alloc.get('category', '').lower() in ai_alloc.get('category', '').lower()
        
        return results
    
    def evaluate_complete_response(self, expected_plans: Dict[str, Dict], ai_response: Dict[str, Any], use_bertscore: bool = False) -> Dict[str, Any]:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (3 ‡πÅ‡∏ú‡∏ô)"""
        results = {
            'overall_metrics': {},
            'plan_metrics': {}
        }
        
        ai_plans = ai_response.get('plans', [])
        
        results['overall_metrics']['expected_plan_count'] = 3
        results['overall_metrics']['actual_plan_count'] = len(ai_plans)
        results['overall_metrics']['has_all_plans'] = len(ai_plans) >= 3
        
        plan_keys = ['plan_1', 'plan_2', 'plan_3']
        all_text_scores = []
        all_numeric_scores = []
        all_multi_level_scores = []  # NEW: Track multi-level metrics

        for i, (plan_key, ai_plan) in enumerate(zip(plan_keys, ai_plans[:3])):
            expected_plan = expected_plans.get(plan_key, {})

            if expected_plan:
                plan_results = self.evaluate_plan(expected_plan, ai_plan, use_bertscore)
                results['plan_metrics'][f'plan_{i+1}'] = plan_results

                if 'text_metrics' in plan_results:
                    all_text_scores.append(plan_results['text_metrics'])
                if 'numeric_metrics' in plan_results:
                    all_numeric_scores.append(plan_results['numeric_metrics'])
                if 'multi_level_metrics' in plan_results:  # NEW: Collect multi-level metrics
                    all_multi_level_scores.append(plan_results['multi_level_metrics'])

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ text metrics
        if all_text_scores:
            avg_text_metrics = {}
            for key in all_text_scores[0].keys():
                values = [s[key] for s in all_text_scores if key in s]
                if values:
                    avg_text_metrics[f'avg_{key}'] = np.mean(values)
            results['overall_metrics']['text_metrics'] = avg_text_metrics

        # NEW: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ multi-level metrics
        if all_multi_level_scores:
            avg_multi_level_metrics = {}
            for key in all_multi_level_scores[0].keys():
                values = [s[key] for s in all_multi_level_scores if key in s and s[key] is not None]
                if values:
                    avg_multi_level_metrics[f'avg_{key}'] = np.mean(values)
            results['overall_metrics']['multi_level_metrics'] = avg_multi_level_metrics

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ numeric metrics
        if all_numeric_scores:
            all_accuracies = []
            for plan_numerics in all_numeric_scores:
                for field, data in plan_numerics.items():
                    if isinstance(data, dict) and 'accuracy' in data:
                        all_accuracies.append(data['accuracy'])

            if all_accuracies:
                results['overall_metrics']['avg_numeric_accuracy'] = np.mean(all_accuracies)

        return results
    
    def print_evaluation_report(self, results: Dict[str, Any], test_case_name: str = "", save_to_file: bool = False, output_dir: Path = None):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢"""
        
        print("\n" + "="*80)
        print(f"{Colors.BOLD}{Colors.CYAN}üìä EVALUATION REPORT{Colors.END}")
        if test_case_name:
            print(f"{Colors.YELLOW}Test Case: {test_case_name}{Colors.END}")
        print("="*80 + "\n")

        # üÜï Legal Compliance Summary (‡πÅ‡∏™‡∏î‡∏á‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å)
        if 'legal_compliance' in results:
            legal = results['legal_compliance']
            has_violations = legal.get('has_violations', False)
            overall_legal_score = legal.get('overall_score', 100)

            print(f"{Colors.BOLD}‚öñÔ∏è  LEGAL COMPLIANCE CHECK{Colors.END}")
            print("‚îÄ" * 80)

            if has_violations:
                print(f"  Status: {Colors.RED}‚ùå FAILED{Colors.END}")
                print(f"  Score:  {Colors.RED}{overall_legal_score}%{Colors.END}")
                print(f"\n  {Colors.RED}üö® Legal Violations Detected:{Colors.END}\n")

                for idx, check in enumerate(legal.get('checks', []), 1):
                    if not check['is_legal']:
                        print(f"  {Colors.RED}Plan {idx}:{Colors.END}")
                        for violation in check['violations']:
                            print(f"    {Colors.RED}‚ùå{Colors.END} {violation['category']}: {violation['recommended_amount']:,} ‡∏ö‡∏≤‡∏ó")
                            print(f"       Legal Max: {violation['legal_max']:,} ‡∏ö‡∏≤‡∏ó")
                            print(f"       Excess: {violation['excess']:,} ‡∏ö‡∏≤‡∏ó ({violation['violation_percentage']:.1f}% over)")
                            print(f"       Reason: {violation['reason']}")
                            print(f"       Law: {violation['law_reference']}")
                            print()
            else:
                print(f"  Status: {Colors.GREEN}‚úÖ PASSED{Colors.END}")
                print(f"  Score:  {Colors.GREEN}{overall_legal_score}%{Colors.END}")
                print(f"  {Colors.GREEN}All plans comply with Thai Tax Law 2568{Colors.END}")

            print()

        # Overall Summary
        if 'overall_metrics' in results:
            overall = results['overall_metrics']
            
            print(f"{Colors.BOLD}üéØ OVERALL SUMMARY{Colors.END}")
            print("‚îÄ" * 80)
            
            # Plans count
            plan_count = overall.get('actual_plan_count', 0)
            expected_count = overall.get('expected_plan_count', 3)
            has_all = overall.get('has_all_plans', False)
            
            status_emoji = "‚úÖ" if has_all else "‚ùå"
            status_color = Colors.GREEN if has_all else Colors.RED
            
            print(f"  Plans Generated: {status_color}{plan_count}/{expected_count} {status_emoji}{Colors.END}")
            
            # Numeric Accuracy
            if 'avg_numeric_accuracy' in overall:
                acc = overall['avg_numeric_accuracy']
                print(f"\n  üí∞ Numeric Accuracy:")
                print(f"     {self.print_progress_bar(acc, 100, 40)}")
                print(f"     Value: {acc:.2f}%")
            
            # Text Similarity
            if 'text_metrics' in overall:
                text_m = overall['text_metrics']
                print(f"\n  üìù Text Similarity:")
                
                if 'avg_rouge1_f1' in text_m:
                    score = text_m['avg_rouge1_f1']
                    color = self.get_score_color(score, 'general')
                    emoji = self.get_score_emoji(score, 'general')
                    print(f"     {emoji} ROUGE-1 F1: {color}{score:.4f}{Colors.END}")
                
                if 'avg_rouge2_f1' in text_m:
                    score = text_m['avg_rouge2_f1']
                    color = self.get_score_color(score, 'general')
                    emoji = self.get_score_emoji(score, 'general')
                    print(f"     {emoji} ROUGE-2 F1: {color}{score:.4f}{Colors.END}")
                
                if 'avg_rougeL_f1' in text_m:
                    score = text_m['avg_rougeL_f1']
                    color = self.get_score_color(score, 'general')
                    emoji = self.get_score_emoji(score, 'general')
                    print(f"     {emoji} ROUGE-L F1: {color}{score:.4f}{Colors.END}")
                
                if 'avg_bleu4' in text_m:
                    score = text_m['avg_bleu4']
                    color = self.get_score_color(score, 'general')
                    emoji = self.get_score_emoji(score, 'general')
                    print(f"     {emoji} BLEU-4:     {color}{score:.4f}{Colors.END}")
                
                if 'avg_bertscore_f1' in text_m:
                    score = text_m['avg_bertscore_f1']
                    color = self.get_score_color(score, 'general')
                    emoji = self.get_score_emoji(score, 'general')
                    print(f"     {emoji} BERTScore:  {color}{score:.4f}{Colors.END}")
            
            print()
        
        # Plan by Plan
        if 'plan_metrics' in results:
            print(f"\n{Colors.BOLD}üìã DETAILED RESULTS BY PLAN{Colors.END}")
            print("="*80 + "\n")
            
            for plan_key, plan_results in results['plan_metrics'].items():
                plan_num = plan_key.split('_')[1]
                print(f"{Colors.BOLD}{Colors.BLUE}‚ñ∂ Plan {plan_num}{Colors.END}")
                print("‚îÄ" * 80)
                
                # Numeric metrics
                if 'numeric_metrics' in plan_results:
                    num_m = plan_results['numeric_metrics']
                    
                    for field, data in num_m.items():
                        if isinstance(data, dict):
                            acc = data['accuracy']
                            expected = data['expected']
                            actual = data['actual']
                            within = data['within_tolerance']
                            
                            status = "‚úÖ" if within else "‚ö†Ô∏è"
                            color = Colors.GREEN if within else Colors.YELLOW
                            
                            field_display = field.replace('_', ' ').title()
                            
                            print(f"\n  {Colors.BOLD}{field_display}:{Colors.END} {status}")
                            print(f"    Expected: {expected:>12,.0f} ‡∏ö‡∏≤‡∏ó")
                            print(f"    Actual:   {actual:>12,.0f} ‡∏ö‡∏≤‡∏ó")
                            print(f"    Accuracy: {color}{acc:.1f}%{Colors.END}")
                
                # Text metrics
                if 'text_metrics' in plan_results:
                    text_m = plan_results['text_metrics']
                    print(f"\n  {Colors.BOLD}Text Similarity:{Colors.END}")
                    
                    if 'rouge1_f1' in text_m:
                        print(f"    ROUGE-1 F1: {text_m['rouge1_f1']:.4f}")
                    if 'bleu4' in text_m:
                        print(f"    BLEU-4:     {text_m['bleu4']:.4f}")
                    if 'bertscore_f1' in text_m:
                        print(f"    BERTScore:  {text_m['bertscore_f1']:.4f}")
                
                # Allocation metrics
                if 'allocation_metrics' in plan_results:
                    alloc_m = plan_results['allocation_metrics']
                    print(f"\n  {Colors.BOLD}Allocation Accuracy:{Colors.END}")
                    
                    inv_acc = alloc_m['average_investment_accuracy']
                    tax_acc = alloc_m['average_tax_saving_accuracy']
                    
                    print(f"    Investment: {self.print_progress_bar(inv_acc, 100, 30)}")
                    print(f"    Tax Saving: {self.print_progress_bar(tax_acc, 100, 30)}")
                
                print()
        
        print("="*80 + "\n")
        
        # Save to file
        if save_to_file and output_dir:
            output_dir.mkdir(parents=True, exist_ok=True)
            report_file = output_dir / f"report_{test_case_name.replace(' ', '_')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"{Colors.GREEN}üíæ Report saved to: {report_file}{Colors.END}\n")
    
    def generate_summary_statistics(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏™‡∏£‡∏∏‡∏õ - ‡∏£‡∏ß‡∏° Multi-Level Metrics"""
        summary = {
            'total_test_cases': len(all_results),
            'text_metrics': {},  # Legacy metrics
            'numeric_metrics': {},
            'multi_level_metrics': {}  # NEW: Multi-level evaluation summary
        }

        # Legacy metrics
        all_rouge1 = []
        all_rouge2 = []
        all_rougeL = []
        all_bleu4 = []
        all_bertscore = []
        all_keyword_match = []
        all_numeric_acc = []

        # Multi-level metrics
        all_desc_rouge1_f1 = []
        all_desc_rougeL_f1 = []
        all_desc_bleu4 = []
        all_desc_bertscore_f1 = []
        all_keyword_coverage = []
        all_semantic_bertscore_f1 = []
        all_keypoint_coverage = []

        for result in all_results:
            if 'overall_metrics' in result:
                overall = result['overall_metrics']

                # Legacy text metrics
                if 'text_metrics' in overall:
                    text_m = overall['text_metrics']
                    if 'avg_rouge1_f1' in text_m:
                        all_rouge1.append(text_m['avg_rouge1_f1'])
                    if 'avg_rouge2_f1' in text_m:
                        all_rouge2.append(text_m['avg_rouge2_f1'])
                    if 'avg_rougeL_f1' in text_m:
                        all_rougeL.append(text_m['avg_rougeL_f1'])
                    if 'avg_bleu4' in text_m:
                        all_bleu4.append(text_m['avg_bleu4'])
                    if 'avg_bertscore_f1' in text_m:
                        all_bertscore.append(text_m['avg_bertscore_f1'])
                    if 'avg_keyword_match_ratio' in text_m:
                        all_keyword_match.append(text_m['avg_keyword_match_ratio'])

                # Multi-level metrics
                if 'multi_level_metrics' in overall:
                    ml_m = overall['multi_level_metrics']
                    if 'avg_desc_rouge1_f1' in ml_m:
                        all_desc_rouge1_f1.append(ml_m['avg_desc_rouge1_f1'])
                    if 'avg_desc_rougeL_f1' in ml_m:
                        all_desc_rougeL_f1.append(ml_m['avg_desc_rougeL_f1'])
                    if 'avg_desc_bleu4' in ml_m:
                        all_desc_bleu4.append(ml_m['avg_desc_bleu4'])
                    if 'avg_desc_bertscore_f1' in ml_m:
                        all_desc_bertscore_f1.append(ml_m['avg_desc_bertscore_f1'])
                    if 'avg_keyword_coverage_ratio' in ml_m:
                        all_keyword_coverage.append(ml_m['avg_keyword_coverage_ratio'])
                    if 'avg_semantic_bertscore_f1' in ml_m:
                        all_semantic_bertscore_f1.append(ml_m['avg_semantic_bertscore_f1'])
                    if 'avg_keypoint_coverage_ratio' in ml_m:
                        all_keypoint_coverage.append(ml_m['avg_keypoint_coverage_ratio'])

                if 'avg_numeric_accuracy' in overall:
                    all_numeric_acc.append(overall['avg_numeric_accuracy'])

        # Legacy metrics aggregation
        if all_rouge1:
            summary['text_metrics']['avg_rouge1_f1'] = np.mean(all_rouge1)
        if all_rouge2:
            summary['text_metrics']['avg_rouge2_f1'] = np.mean(all_rouge2)
        if all_rougeL:
            summary['text_metrics']['avg_rougeL_f1'] = np.mean(all_rougeL)
        if all_bleu4:
            summary['text_metrics']['avg_bleu4'] = np.mean(all_bleu4)
        if all_bertscore:
            summary['text_metrics']['avg_bertscore_f1'] = np.mean(all_bertscore)
        if all_keyword_match:
            summary['text_metrics']['avg_keyword_match_ratio'] = np.mean(all_keyword_match)

        # Multi-level metrics aggregation
        if all_desc_rouge1_f1:
            summary['multi_level_metrics']['avg_desc_rouge1_f1'] = np.mean(all_desc_rouge1_f1)
        if all_desc_rougeL_f1:
            summary['multi_level_metrics']['avg_desc_rougeL_f1'] = np.mean(all_desc_rougeL_f1)
        if all_desc_bleu4:
            summary['multi_level_metrics']['avg_desc_bleu4'] = np.mean(all_desc_bleu4)
        if all_desc_bertscore_f1:
            summary['multi_level_metrics']['avg_desc_bertscore_f1'] = np.mean(all_desc_bertscore_f1)
        if all_keyword_coverage:
            summary['multi_level_metrics']['avg_keyword_coverage_ratio'] = np.mean(all_keyword_coverage)
        if all_semantic_bertscore_f1:
            summary['multi_level_metrics']['avg_semantic_bertscore_f1'] = np.mean(all_semantic_bertscore_f1)
        if all_keypoint_coverage:
            summary['multi_level_metrics']['avg_keypoint_coverage_ratio'] = np.mean(all_keypoint_coverage)

        # Numeric metrics
        if all_numeric_acc:
            summary['numeric_metrics']['avg_accuracy'] = np.mean(all_numeric_acc)
            summary['numeric_metrics']['min_accuracy'] = np.min(all_numeric_acc)
            summary['numeric_metrics']['max_accuracy'] = np.max(all_numeric_acc)

        return summary
    
    def print_summary_report(self, summary: Dict[str, Any]):
        """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°"""
        
        print("\n" + "="*80)
        print(f"{Colors.BOLD}{Colors.HEADER}üìà FINAL SUMMARY STATISTICS{Colors.END}")
        print("="*80 + "\n")
        
        total = summary.get('total_test_cases', 0)
        print(f"{Colors.BOLD}Total Test Cases:{Colors.END} {Colors.CYAN}{total}{Colors.END}\n")
        
        # Multi-Level Metrics (PRIMARY METRICS - MOST IMPORTANT!)
        if 'multi_level_metrics' in summary and summary['multi_level_metrics']:
            print(f"{Colors.BOLD}{Colors.CYAN}üéØ DESCRIPTION TEXT MATCHING (Primary Metric):{Colors.END}")
            print("=" * 80)
            print(f"{Colors.CYAN}‚ú® These metrics show how well the AI matched the EXACT description text ‚ú®{Colors.END}")
            print("=" * 80)

            ml = summary['multi_level_metrics']

            # Show BLEU-4 prominently (most important)
            if 'avg_desc_bleu4' in ml:
                val = ml['avg_desc_bleu4']
                color = Colors.GREEN if val >= 0.9 else Colors.YELLOW if val >= 0.7 else Colors.RED
                emoji = "‚úÖ" if val >= 0.9 else "‚ö†Ô∏è" if val >= 0.7 else "‚ùå"
                print(f"\n  {emoji} {Colors.BOLD}BLEU-4 (Description Text)    : {color}{val:.4f} = {val*100:.1f}%{Colors.END}")

            # Show BERTScore
            if 'avg_desc_bertscore_f1' in ml:
                val = ml['avg_desc_bertscore_f1']
                color = Colors.GREEN if val >= 0.9 else Colors.YELLOW if val >= 0.7 else Colors.RED
                emoji = "‚úÖ" if val >= 0.9 else "‚ö†Ô∏è" if val >= 0.7 else "‚ùå"
                print(f"  {emoji} {Colors.BOLD}BERTScore (Description Text) : {color}{val:.4f} = {val*100:.1f}%{Colors.END}")

            # Show ROUGE scores with note
            if 'avg_desc_rouge1_f1' in ml:
                val = ml['avg_desc_rouge1_f1']
                color = Colors.GREEN if val >= 0.7 else Colors.YELLOW if val >= 0.5 else Colors.RED
                emoji = "‚úÖ" if val >= 0.7 else "‚ö†Ô∏è" if val >= 0.5 else "‚ö†Ô∏è"
                note = f" {Colors.YELLOW}(Note: May be 0 due to Thai tokenization){Colors.END}" if val == 0 else ""
                print(f"  {emoji} ROUGE-1 F1 (Description)     : {color}{val:.4f}{Colors.END}{note}")

            if 'avg_desc_rougeL_f1' in ml:
                val = ml['avg_desc_rougeL_f1']
                color = Colors.GREEN if val >= 0.7 else Colors.YELLOW if val >= 0.5 else Colors.RED
                emoji = "‚úÖ" if val >= 0.7 else "‚ö†Ô∏è" if val >= 0.5 else "‚ö†Ô∏è"
                note = f" {Colors.YELLOW}(Note: May be 0 due to Thai tokenization){Colors.END}" if val == 0 else ""
                print(f"  {emoji} ROUGE-L F1 (Description)     : {color}{val:.4f}{Colors.END}{note}")

            print(f"\n{Colors.BOLD}üìä Supporting Metrics:{Colors.END}")
            print("‚îÄ" * 80)

            if 'avg_keyword_coverage_ratio' in ml:
                val = ml['avg_keyword_coverage_ratio']
                color = self.get_score_color(val, 'general')
                emoji = self.get_score_emoji(val, 'general')
                print(f"  {emoji} Keyword Coverage          : {color}{val:.2%}{Colors.END}")

            if 'avg_semantic_bertscore_f1' in ml:
                val = ml['avg_semantic_bertscore_f1']
                color = self.get_score_color(val, 'general')
                emoji = self.get_score_emoji(val, 'general')
                print(f"  {emoji} Semantic Similarity       : {color}{val:.4f}{Colors.END}")

            if 'avg_keypoint_coverage_ratio' in ml:
                val = ml['avg_keypoint_coverage_ratio']
                color = self.get_score_color(val, 'general')
                emoji = self.get_score_emoji(val, 'general')
                print(f"  {emoji} Key Points Coverage       : {color}{val:.2%}{Colors.END}")
            print()

        # Legacy Text Metrics (LESS RELEVANT - Shown for reference only)
        if 'text_metrics' in summary and summary['text_metrics']:
            print(f"{Colors.BOLD}{Colors.YELLOW}‚ö†Ô∏è  Legacy Full-Text Metrics (Less Relevant - For Reference Only):{Colors.END}")
            print("‚îÄ" * 80)
            print(f"{Colors.YELLOW}Note: These compare ALL text including allocations (pros/cons) that we don't control{Colors.END}")
            print(f"{Colors.YELLOW}      Focus on the 'Description Text Matching' metrics above instead!{Colors.END}")
            print("‚îÄ" * 80)

            for key, value in summary['text_metrics'].items():
                metric_name = key.replace('avg_', '').replace('_', '-').upper()
                color = self.get_score_color(value, 'general')
                emoji = self.get_score_emoji(value, 'general')
                print(f"  {emoji} {metric_name:25} : {color}{value:.4f}{Colors.END}")
            print()

        # Numeric Metrics
        if 'numeric_metrics' in summary and summary['numeric_metrics']:
            print(f"{Colors.BOLD}üí∞ Numeric Accuracy:{Colors.END}")
            print("‚îÄ" * 80)

            for key, value in summary['numeric_metrics'].items():
                metric_name = key.replace('_', ' ').title()
                color = self.get_score_color(value, 'accuracy')
                emoji = self.get_score_emoji(value, 'accuracy')
                print(f"  {emoji} {metric_name:15} : {color}{value:.2f}%{Colors.END}")
            print()

        print("="*80 + "\n")