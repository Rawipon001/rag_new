"""
Evaluation Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á AI
‡πÉ‡∏ä‡πâ ROUGE, BLEU, ‡πÅ‡∏•‡∏∞ BERTScore
"""

from typing import Dict, List, Any
import numpy as np

# ROUGE
from rouge_score import rouge_scorer

# BLEU
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# BERTScore
from bert_score import score as bert_score

# Thai tokenizer
from pythainlp.tokenize import word_tokenize


class EvaluationService:
    """
    Service ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
    """
    
    def __init__(self):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ROUGE scorer
        self.rouge_scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=False
        )
        
        # BLEU smoothing function
        self.smoothing = SmoothingFunction().method1
    
    def tokenize_thai(self, text: str) -> List[str]:
        """
        Tokenize ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        
        Args:
            text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
            
        Returns:
            List of tokens
        """
        return word_tokenize(text, engine='newmm')
    
    def calculate_rouge(
        self, 
        reference: str, 
        hypothesis: str
    ) -> Dict[str, float]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE scores
        
        ROUGE-1: Unigram overlap
        ROUGE-2: Bigram overlap
        ROUGE-L: Longest common subsequence
        
        Args:
            reference: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (ground truth)
            hypothesis: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á ROUGE scores
        """
        scores = self.rouge_scorer.score(reference, hypothesis)
        
        return {
            'rouge1_precision': scores['rouge1'].precision,
            'rouge1_recall': scores['rouge1'].recall,
            'rouge1_f1': scores['rouge1'].fmeasure,
            'rouge2_precision': scores['rouge2'].precision,
            'rouge2_recall': scores['rouge2'].recall,
            'rouge2_f1': scores['rouge2'].fmeasure,
            'rougeL_precision': scores['rougeL'].precision,
            'rougeL_recall': scores['rougeL'].recall,
            'rougeL_f1': scores['rougeL'].fmeasure,
        }
    
    def calculate_bleu(
        self, 
        reference: str, 
        hypothesis: str
    ) -> Dict[str, float]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BLEU score
        
        BLEU ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á n-grams ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
        
        Args:
            reference: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            hypothesis: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á BLEU scores
        """
        # Tokenize
        ref_tokens = self.tokenize_thai(reference)
        hyp_tokens = self.tokenize_thai(hypothesis)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BLEU-1, BLEU-2, BLEU-3, BLEU-4
        bleu1 = sentence_bleu(
            [ref_tokens], hyp_tokens, 
            weights=(1, 0, 0, 0),
            smoothing_function=self.smoothing
        )
        bleu2 = sentence_bleu(
            [ref_tokens], hyp_tokens, 
            weights=(0.5, 0.5, 0, 0),
            smoothing_function=self.smoothing
        )
        bleu3 = sentence_bleu(
            [ref_tokens], hyp_tokens, 
            weights=(0.33, 0.33, 0.33, 0),
            smoothing_function=self.smoothing
        )
        bleu4 = sentence_bleu(
            [ref_tokens], hyp_tokens, 
            weights=(0.25, 0.25, 0.25, 0.25),
            smoothing_function=self.smoothing
        )
        
        return {
            'bleu1': bleu1,
            'bleu2': bleu2,
            'bleu3': bleu3,
            'bleu4': bleu4,
        }
    
    def calculate_bertscore(
        self, 
        references: List[str], 
        hypotheses: List[str],
        lang: str = 'th'
    ) -> Dict[str, float]:
        """
        ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore
        
        BERTScore ‡πÉ‡∏ä‡πâ BERT embeddings ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
        
        Args:
            references: List ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            hypotheses: List ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
            lang: ‡∏†‡∏≤‡∏©‡∏≤ (th=‡πÑ‡∏ó‡∏¢, en=‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á BERTScore (Precision, Recall, F1)
        """
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore
        P, R, F1 = bert_score(
            hypotheses, 
            references, 
            lang=lang,
            verbose=False,
            model_type='xlm-roberta-base'  # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏£‡∏ß‡∏°‡πÑ‡∏ó‡∏¢
        )
        
        return {
            'bertscore_precision': P.mean().item(),
            'bertscore_recall': R.mean().item(),
            'bertscore_f1': F1.mean().item(),
        }
    
    def evaluate_single(
        self, 
        reference: str, 
        hypothesis: str,
        use_bertscore: bool = True
    ) -> Dict[str, Any]:
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        
        Args:
            reference: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            hypothesis: ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
            use_bertscore: ‡πÉ‡∏ä‡πâ BERTScore ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô)
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á metrics ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        results = {}
        
        # ROUGE
        rouge_scores = self.calculate_rouge(reference, hypothesis)
        results.update(rouge_scores)
        
        # BLEU
        bleu_scores = self.calculate_bleu(reference, hypothesis)
        results.update(bleu_scores)
        
        # BERTScore (optional - ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô)
        if use_bertscore:
            bert_scores = self.calculate_bertscore([reference], [hypothesis])
            results.update(bert_scores)
        
        return results
    
    def evaluate_batch(
        self, 
        references: List[str], 
        hypotheses: List[str],
        use_bertscore: bool = True
    ) -> Dict[str, Any]:
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
        
        Args:
            references: List ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            hypotheses: List ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å AI
            use_bertscore: ‡πÉ‡∏ä‡πâ BERTScore ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á metrics ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
        """
        all_rouge1_f1 = []
        all_rouge2_f1 = []
        all_rougeL_f1 = []
        all_bleu4 = []
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡∏π‡πà
        for ref, hyp in zip(references, hypotheses):
            # ROUGE
            rouge_scores = self.calculate_rouge(ref, hyp)
            all_rouge1_f1.append(rouge_scores['rouge1_f1'])
            all_rouge2_f1.append(rouge_scores['rouge2_f1'])
            all_rougeL_f1.append(rouge_scores['rougeL_f1'])
            
            # BLEU
            bleu_scores = self.calculate_bleu(ref, hyp)
            all_bleu4.append(bleu_scores['bleu4'])
        
        results = {
            'rouge1_f1_avg': np.mean(all_rouge1_f1),
            'rouge2_f1_avg': np.mean(all_rouge2_f1),
            'rougeL_f1_avg': np.mean(all_rougeL_f1),
            'bleu4_avg': np.mean(all_bleu4),
        }
        
        # BERTScore (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏±‡πâ‡∏á batch - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤)
        if use_bertscore:
            bert_scores = self.calculate_bertscore(references, hypotheses)
            results.update(bert_scores)
        
        return results
    
    def evaluate_recommendation(
        self,
        reference_recommendation: Dict[str, Any],
        ai_recommendation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏†‡∏≤‡∏©‡∏µ (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ project ‡∏ô‡∏µ‡πâ)
        
        Args:
            reference_recommendation: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
            ai_recommendation: ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏à‡∏≤‡∏Å AI
            
        Returns:
            Dict ‡∏Ç‡∏≠‡∏á metrics + accuracy ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        """
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        ref_text = f"{reference_recommendation.get('strategy', '')} {reference_recommendation.get('description', '')}"
        hyp_text = f"{ai_recommendation.get('strategy', '')} {ai_recommendation.get('description', '')}"
        
        text_scores = self.evaluate_single(ref_text, hyp_text, use_bertscore=False)
        
        # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
        numeric_accuracy = {}
        
        numeric_fields = [
            'investment_amount',
            'tax_saving',
            'expected_return_1y',
            'expected_return_3y',
            'expected_return_5y'
        ]
        
        for field in numeric_fields:
            ref_val = reference_recommendation.get(field, 0)
            ai_val = ai_recommendation.get(field, 0)
            
            if ref_val > 0:
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì percentage error
                error = abs(ref_val - ai_val) / ref_val * 100
                accuracy = max(0, 100 - error)
                numeric_accuracy[f'{field}_accuracy'] = accuracy
        
        # ‡∏£‡∏ß‡∏° scores
        results = {**text_scores, **numeric_accuracy}
        
        return results
    
    def print_evaluation_report(self, results: Dict[str, Any]):
        """
        ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        
        Args:
            results: ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô
        """
        print("\n" + "="*60)
        print("üìä EVALUATION REPORT")
        print("="*60)
        
        # ROUGE
        if 'rouge1_f1' in results:
            print("\nüî¥ ROUGE Scores:")
            print(f"  ROUGE-1 F1: {results.get('rouge1_f1', 0):.4f}")
            print(f"  ROUGE-2 F1: {results.get('rouge2_f1', 0):.4f}")
            print(f"  ROUGE-L F1: {results.get('rougeL_f1', 0):.4f}")
        
        # BLEU
        if 'bleu4' in results:
            print("\nüîµ BLEU Scores:")
            print(f"  BLEU-1: {results.get('bleu1', 0):.4f}")
            print(f"  BLEU-2: {results.get('bleu2', 0):.4f}")
            print(f"  BLEU-3: {results.get('bleu3', 0):.4f}")
            print(f"  BLEU-4: {results.get('bleu4', 0):.4f}")
        
        # BERTScore
        if 'bertscore_f1' in results:
            print("\nüü¢ BERTScore:")
            print(f"  Precision: {results.get('bertscore_precision', 0):.4f}")
            print(f"  Recall: {results.get('bertscore_recall', 0):.4f}")
            print(f"  F1: {results.get('bertscore_f1', 0):.4f}")
        
        # Numeric Accuracy
        accuracy_keys = [k for k in results.keys() if k.endswith('_accuracy')]
        if accuracy_keys:
            print("\nüí∞ Numeric Accuracy:")
            for key in accuracy_keys:
                field = key.replace('_accuracy', '')
                print(f"  {field}: {results[key]:.2f}%")
        
        print("\n" + "="*60)


# ==========================================
# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
# ==========================================

if __name__ == "__main__":
    evaluator = EvaluationService()
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 1: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    print("\nüìù Example 1: Single Evaluation")
    
    reference = "‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF 200,000 ‡∏ö‡∏≤‡∏ó ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏´‡∏¢‡πà‡∏≠‡∏ô‡∏†‡∏≤‡∏©‡∏µ‡πÑ‡∏î‡πâ 50,000 ‡∏ö‡∏≤‡∏ó"
    hypothesis = "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 200,000 ‡∏ö‡∏≤‡∏ó ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏†‡∏≤‡∏©‡∏µ‡πÑ‡∏î‡πâ 50,000 ‡∏ö‡∏≤‡∏ó"
    
    scores = evaluator.evaluate_single(reference, hypothesis, use_bertscore=False)
    evaluator.print_evaluation_report(scores)
    
    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
    print("\nüìù Example 2: Recommendation Evaluation")
    
    reference_rec = {
        "strategy": "‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF 200,000 ‡∏ö‡∏≤‡∏ó",
        "description": "‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô RMF ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡∏™‡∏°",
        "investment_amount": 200000,
        "tax_saving": 50000,
        "expected_return_1y": 5.5,
        "expected_return_3y": 6.8,
        "expected_return_5y": 8.0,
    }
    
    ai_rec = {
        "strategy": "‡∏•‡∏á‡∏ó‡∏∏‡∏ô RMF 200,000 ‡∏ö‡∏≤‡∏ó",
        "description": "‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô RMF ‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô‡∏ú‡∏™‡∏°",
        "investment_amount": 200000,
        "tax_saving": 48000,  # ‡∏ú‡∏¥‡∏î‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢
        "expected_return_1y": 5.8,  # ‡∏ú‡∏¥‡∏î‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢
        "expected_return_3y": 6.5,
        "expected_return_5y": 8.2,
    }
    
    rec_scores = evaluator.evaluate_recommendation(reference_rec, ai_rec)
    evaluator.print_evaluation_report(rec_scores)