# 3.7 หลักการประเมินความสามารถของ AI

การประเมินความสามารถของ AI จำเป็นต้องวัดผลในหลายมิติเพื่อให้มั่นใจว่าระบบสามารถให้คำแนะนำที่ถูกต้อง ปลอดภัย และมีคุณภาพ การประเมินครอบคลุม 3 มิติหลัก คือ:

1. **ความถูกต้องตามกฎหมาย (Legal Compliance)**: วัดความสอดคล้องกับพระราชบัญญัติภาษีเงินได้ พ.ศ. 2568
2. **ความแม่นยำทางตัวเลข (Numeric Accuracy)**: วัดความถูกต้องของการคำนวณภาษี เช่น อัตราลดหย่อน ภาษีที่ประหยัดได้
3. **คุณภาพข้อความ (Text Quality)**: ประเมินคุณภาพคำอธิบายที่ AI สร้างว่าถูกต้องและคล้ายคลึงกับข้อมูลใน Knowledge Base มากน้อยเพียงใด

---

## 3.7.1 ความถูกต้องตามกฎหมาย (Legal Compliance Evaluation)

### วัตถุประสงค์
ตรวจสอบว่าคำแนะนำของ AI สอดคล้องกับกฎหมายภาษีไทยหรือไม่ โดยเฉพาะการลดหย่อนภาษีแต่ละหมวด

### รายการตรวจสอบ (8 หมวด)

| หมวด | ขีดจำกัดตามกฎหมาย | ฐานกฎหมาย |
|------|-------------------|-----------|
| RMF | ≤ min(30% รายได้, 500,000) | มาตรา 42(12) |
| ประกันชีวิต | ≤ 100,000 | มาตรา 42(8) |
| ประกันสุขภาพ | ≤ 25,000 | มาตรา 42(9) |
| ประกันบำนาญ | ≤ min(15% รายได้, 200,000) | มาตรา 42(13) |
| ThaiESG | ≤ min(30% รายได้, 300,000) | มาตรา 42(21) |
| PVD | ≤ min(15% รายได้, 500,000) | มาตรา 42(11) |
| GPF | ≤ min(30% รายได้, 500,000) | มาตรา 42(10) |
| รวมประกันชีวิต+สุขภาพ | ≤ 125,000 | มาตรา 42(8)(9) |

### วิธีการประเมิน
ใช้ **Binary Scoring (Pass/Fail)**:
```
Score = {
    100%  ถ้า violations = 0
    0%    ถ้า violations ≥ 1
}
```
**เหตุผล**: กฎหมายเป็น hard constraint ไม่สามารถละเมิดได้ แม้เพียง 1 ข้อ ตามมาตรฐาน SEC, FINRA [1][2]

### เกณฑ์การยอมรับ
- **ผ่าน (100%)**: ถูกต้องทุกข้อ
- **ไม่ผ่าน (0%)**: มี violation ≥ 1 ข้อ → ห้ามใช้งาน

---

## 3.7.2 ความแม่นยำทางตัวเลข (Numeric Accuracy Evaluation)

### วัตถุประสงค์
วัดความแม่นยำของตัวเลขที่ AI คำนวณ เช่น total_investment, total_tax_saving

### วิธีการประเมิน

ใช้ **MAPE (Mean Absolute Percentage Error)** [5]:

```
Error% = |Expected - Actual| / Expected × 100%
Accuracy = (1 - Error%) × 100%
```

**เหตุผล**:
- Scale-independent: เปรียบเทียบได้ทุกระดับรายได้
- ตีความง่าย: Accuracy 95% = ผิดพลาด 5%
- มาตรฐาน Financial Systems (ISO 20022 [7], IEEE 754 [8])

### Tolerance Level

| Level | Tolerance | ใช้สำหรับ | อ้างอิง |
|-------|-----------|----------|---------|
| **Standard** | **±10%** | **Production (โครงการนี้)** | IEEE 754 [8], FPA [9] |
| Strict | ±5% | Banking, Accounting | ISO 20022 [7] |
| Relaxed | ±15% | Development/Testing | NIST AI RMF [10] |

### เกณฑ์การยอมรับ

| Accuracy | Grade | การดำเนินการ |
|----------|-------|--------------|
| ≥ 95% | A | ใช้ได้ (Excellent) |
| 90-94% | B | ใช้ได้ (Good) |
| 85-89% | C | ใช้ได้ (Acceptable) |
| < 85% | F | ไม่ผ่าน (ต้องแก้ไข) |

---

## 3.7.3 คุณภาพข้อความ (Text Quality Evaluation)

### วัตถุประสงค์
ประเมินคุณภาพคำอธิบายที่ AI สร้างให้ผู้ใช้ว่ามีความถูกต้อง ครบถ้วน และมีความหมายตรงกับ Knowledge Base

### เครื่องมือวัด 3 ตัว

ใช้ **3 metrics** เพราะวัดคนละมิติ:

| Metric | วัดอะไร | ทำไมสำคัญสำหรับ Tax | อ้างอิง |
|--------|---------|---------------------|---------|
| **BLEU** | ความแม่นยำของคำศัพท์ (Precision) | ศัพท์เทคนิค เช่น "ลดหย่อน" ต้องใช้ถูก | [13] |
| **ROUGE** | ความครบถ้วนของเนื้อหา (Recall) | ข้อมูลสำคัญ เช่น 30%, 5 ปี ห้ามขาด | [14] |
| **BERTScore** | ความหมายทางความหมาย (Semantic) | จับ synonyms เช่น "ลดหย่อน" ≈ "ประหยัด" | [15] |

### 3.7.3.1 BLEU Score

**คำจำกัดความ**: วัดความคล้ายคลึงของคำโดยใช้ n-gram matching (Papineni et al., 2002 [13])

**สูตร**:
```
BLEU = วัด n-gram (1,2,3,4) ที่ตรงกันระหว่าง AI output และ reference
เน้น: Precision (ใช้คำถูกหรือไม่)
```

**ตัวอย่าง**:
```
Reference: "RMF ลดหย่อนภาษีได้ 30% ของรายได้"
Good (BLEU 70%): "RMF ช่วยลดหย่อนภาษี 30% ของรายได้"
Poor (BLEU 30%): "RMF ดีสำหรับประหยัดภาษี" (คำไม่ตรง)
```

**เกณฑ์**: ≥ 50%

### 3.7.3.2 ROUGE Score

**คำจำกัดความ**: วัดความครบถ้วนของเนื้อหาเทียบกับ reference (Lin, 2004 [14])

**สูตร**:
```
ROUGE = วัดว่า AI output ครอบคลุมข้อมูลจาก reference มากน้อยแค่ไหน
เน้น: Recall (มีข้อมูลครบหรือไม่)
```

**ตัวอย่าง**:
```
Reference: "RMF ลดภาษีได้ 30% หรือ 500,000 บาท ต้องถือ 5 ปี"
Good (ROUGE 80%): "RMF ลดภาษี 30% หรือสูงสุด 500,000 ต้องถือครบ 5 ปี" (ครบถ้วน)
Poor (ROUGE 40%): "RMF ช่วยลดภาษีได้" (ขาด 30%, 500K, 5 ปี)
```

**เกณฑ์**: ≥ 60%

### 3.7.3.3 BERTScore

**คำจำกัดความ**: วัดความคล้ายคลึงทางความหมายโดยใช้ BERT embeddings (Zhang et al., 2020 [15])

**สูตร**:
```
BERTScore = คำนวณ cosine similarity ระหว่าง embeddings
เน้น: Semantic Similarity (ความหมายตรงหรือไม่)
```

**ตัวอย่าง**:
```
Reference: "RMF ช่วยลดหย่อนภาษี"
Good (BERTScore 0.88): "RMF ช่วยประหยัดภาษี"
  → แม้คำต่าง แต่ความหมายเหมือนกัน ("ลดหย่อน" ≈ "ประหยัด")
```

**เกณฑ์**: ≥ 0.80

### สรุป: ทำไมต้องใช้ทั้ง 3 ตัว

| Metric | ถ้าไม่มี จะเกิดอะไร |
|--------|-------------------|
| **BLEU** | อาจใช้ศัพท์ผิด เช่น "หัก" แทน "ลดหย่อน" |
| **ROUGE** | อาจขาดข้อมูลสำคัญ เช่น ไม่บอก 5 ปี |
| **BERTScore** | อาจจับ synonym ไม่ได้ เช่น "ประหยัด" vs "ลดหย่อน" |

**การคำนวณรวม**:
```
Text Quality Score = (BLEU + ROUGE + BERTScore) / 3
เกณฑ์ผ่าน: ≥ 65%
```

### 3.7.3.4 Key Points Coverage

**วัตถุประสงค์**: ตรวจสอบว่ามีข้อมูล must-have ครบหรือไม่

**ตัวอย่าง Key Points สำหรับ RMF**:
1. นิยาม (กองทุนเลี้ยงชีพ)
2. ประโยชน์ (ลดหย่อนภาษี)
3. ขีดจำกัด (30% และ 500,000)
4. เงื่อนไข (ถือ 5 ปี)
5. ข้อจำกัด (penalty ถ้าไถ่ถอนก่อน)

**การคำนวณ**:
```
Coverage = (จำนวน key points ที่มี / ทั้งหมด) × 100%
เกณฑ์ผ่าน: ≥ 80%
```

---

## 3.7.4 กรอบการประเมินโดยรวม

### การคำนวณคะแนนรวม

```
Overall Score = (0.50 × Legal) + (0.30 × Numeric) +
                (0.15 × Text) + (0.05 × Key Points)
```

**เหตุผลน้ำหนัก**:
- **Legal 50%**: ผิดกฎหมาย = ใช้ไม่ได้เลย (สำคัญสุด)
- **Numeric 30%**: ผิดตัวเลข = แผนล้มเหลว
- **Text 15%**: อธิบายไม่ดี = ผู้ใช้ไม่เข้าใจ
- **Key Points 5%**: ขาดข้อมูลสำคัญ

### เกณฑ์การยอมรับ

**Hard Requirements**:
```
✅ Legal = 100% (บังคับ)
✅ Numeric ≥ 85% (บังคับ)
✅ Text ≥ 65% (แนะนำ)
✅ Key Points ≥ 80% (แนะนำ)

ถ้า Legal ≠ 100% → Overall = 0% (Auto-fail)
```

### Decision Matrix

| Legal | Numeric | Text | Key Points | Overall | สถานะ |
|-------|---------|------|------------|---------|-------|
| 100% | ≥ 95% | ≥ 70% | ≥ 90% | ≥ 95% | ✅ Excellent |
| 100% | ≥ 90% | ≥ 65% | ≥ 80% | 85-95% | ✅ Good |
| 100% | ≥ 85% | ≥ 60% | ≥ 80% | 75-85% | ⚠️ Fair |
| < 100% | Any | Any | Any | 0% | ❌ Fail |

### ตัวอย่างการคำนวณ

```
ผลการประเมิน:
  Legal:       100% (ผ่านทุกข้อ)
  Numeric:     98%  (แม่นยำมาก)
  Text:        75%  (BLEU 70%, ROUGE 78%, BERT 0.85)
  Key Points:  90%  (ครบ 9/10)

Overall = (0.50 × 100) + (0.30 × 98) + (0.15 × 75) + (0.05 × 90)
        = 50 + 29.4 + 11.25 + 4.5
        = 95.15% ✅ Excellent
```

---

## 3.7.5 สรุป

### ลำดับความสำคัญ

```
1. Legal Compliance (50%) → ต้อง 100% ไม่มีข้อยกเว้น
2. Numeric Accuracy (30%) → ต้อง ≥ 85%
3. Text Quality (15%) → ควร ≥ 65%
4. Key Points (5%) → ควร ≥ 80%
```

### Checklist สำหรับ Production

```
✅ Legal = 100%
✅ Numeric ≥ 90%
✅ Text ≥ 65% (BLEU ≥50%, ROUGE ≥60%, BERT ≥0.80)
✅ Key Points ≥ 80%
✅ Overall ≥ 90%
✅ ทดสอบกับ 7+ test cases
```

---

## เอกสารอ้างอิง

[1] U.S. Securities and Exchange Commission (SEC). (2021). "Compliance and Disclosure Interpretations."

[2] Financial Industry Regulatory Authority (FINRA). (2022). "Regulatory Notice 22-08."

[5] Hyndman, R. J., & Koehler, A. B. (2006). "Another look at measures of forecast accuracy." *International Journal of Forecasting*, 22(4), 679-688.

[7] ISO 20022. (2013). "Financial services — Universal financial industry message scheme."

[8] IEEE 754-2019. "IEEE Standard for Floating-Point Arithmetic."

[9] Financial Planning Association (FPA). (2020). "Best Practices in Financial Planning."

[10] NIST. (2023). "Artificial Intelligence Risk Management Framework (AI RMF 1.0)."

[13] Papineni, K., et al. (2002). "BLEU: a Method for Automatic Evaluation of Machine Translation." *ACL*.

[14] Lin, C. Y. (2004). "ROUGE: A Package for Automatic Evaluation of Summaries." *ACL*.

[15] Zhang, T., et al. (2020). "BERTScore: Evaluating Text Generation with BERT." *ICLR*.

[16] กรมสรรพากร. (2568). "พระราชบัญญัติภาษีเงินได้ พ.ศ. 2568."

---

**สิ้นสุดเอกสาร**
